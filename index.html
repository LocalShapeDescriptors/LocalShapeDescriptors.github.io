<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <meta name="theme-color" content="#ffffff" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141682504-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141682504-1');
  </script>


  <!-- SEO -->
  <meta property="og:title" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Auxiliary learning for large scale connectomics" />
  <meta property="og:image" content="assets/img/lsds_header.jpeg" />
  <meta property="og:url" content="https://localshapedescriptors.github.io" />

  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta name="twitter:image" content="assets/img/lsds_header.jpeg" />

</head>

<style>

body {
  margin: 0px;
}

.row {
  display: flex;
  flex-wrap: wrap;
  padding: 0 4px;
}

/* Create four equal columns that sits next to each other */
.column {
  flex: 25%;
  max-width: 25%;
  padding: 0 4px;
}

.column img {
  vertical-align: middle;
  width: 100%;
}

/* Responsive layout - makes a two column-layout instead of four columns */
@media screen and (max-width: 800px) {
  .column {
    flex: 50%;
    max-width: 50%;
  }
}

/* Responsive layout - makes the two columns stack on top of each other instead of next to each other */
@media screen and (max-width: 600px) {
  .column {
    flex: 100%;
    max-width: 100%;
  }
}

.image-grid {
  display: flex;
  flex-wrap: wrap;
  width: 90%;
  margin: 0 auto;
}
.grid-image {
  display: block;
  flex-basis: 100%;
  width: 100%;
  height: 100%;
  padding: 10px;
  box-sizing: border-box;
}

@media only screen and (min-width: 640px) {
  .grid-image {
    flex-basis: 50%;
  }
}

@media only screen and (min-width: 960px) {
  .grid-image {
    flex-basis: 33.333%;
  }
}

@media only screen and (min-width: 1280px) {
  .grid-image {
    flex-basis: 25%;
  }
}

@media only screen and (min-width: 1600px) {
  .grid-image {
    flex-basis: 20%;
  }
}

.a {
  position: absolute;
  z-index: 3;
}

.b {
  position: absolute;
  z-index: 2;
}

.img-magnifier-glass {
  position: absolute;
  border: 2px solid #000;
  border-radius: 50%;
  cursor: none;
  /*Set the size of the magnifier glass:*/
  width: 50px;
  height: 50px;
  z-index: 1;
}

.accordion-container {
  position: relative;
  width: 100%;
  overflow: hidden;
  margin: auto;
}

.responsive-container {
  position: relative;
  width: 100%;
  overflow: hidden;
  margin: auto;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
}

.test-container {
  position: relative;
  width: 100%;
  height: 100%;
  overflow: hidden;
  margin: auto;
  border: 5px solid red;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
}

.plot-container {
  position: relative;
  width: 100%;
  height: 650px;
  overflow: hidden;
  margin: auto;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  display: block;
  border: none;
}

.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}

.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}

.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}

.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

.accordion {
  margin: auto;
  position: relative;
}
.accordion input {
  display: none;
}

.box {
  position: relative;
  background: white;
  height: 64px;
  transition: all .15s ease-in-out;
}

.box::before {
  content: '';
  position: absolute;
  display: block;
  top: 0;
  bottom: 0;
  left: 0;
  right: 0;
  pointer-events: none;
  box-shadow: 0 -1px 0 #e5e5e5,0 0 2px rgba(0,0,0,.12),0 2px 4px rgba(0,0,0,.24);
}

header.box {
  background: #00BCD4;
  z-index: 100;
  cursor: initial;
  box-shadow: 0 -1px 0 #e5e5e5,0 0 2px -2px rgba(0,0,0,.12),0 2px 4px -4px rgba(0,0,0,.24);
}

header.box-title {
  margin: 0;
  font-weight: normal;
  font-size: 16pt;
  color: white;
  cursor: initial;
}

.box-title {
  width: calc(100% - 40px);
  height: 64px;
  line-height: 64px;
  padding: 0 20px;
  display: inline-block;
  cursor: pointer;
  -webkit-touch-callout: none;-webkit-user-select: none;-khtml-user-select: none;-moz-user-select: none;-ms-user-select: none;user-select: none;
}

.box-content {
  width: 100%;
  height: 100%;
  display: none;
  overflow: hidden;
  margin: auto;
}

.box-close {
  position: absolute;
  height: 64px;
  width: 100%;
  top: 0;
  left: 0;
  cursor: pointer;
  display: none;
}
input:checked + .box {
  height: auto;
  margin: 16px 0;
  box-shadow: 0 0 6px rgba(0,0,0,.16),0 6px 12px rgba(0,0,0,.32);
}
input:checked + .box .box-title {
  border-bottom: 1px solid rgba(0,0,0,.18);
}
input:checked + .box .box-content,
input:checked + .box .box-close {
  display: inline-block;
}

.arrows section .box-title {
  padding-left: 44px;
  width: calc(100% - 64px);
}
.arrows section .box-title:before {
  position: absolute;
  display: block;
  content: '\203a';
  font-size: 18pt;
  left: 20px;
  top: -2px;
  transition: transform .15s ease-in-out;
  color: rgba(0,0,0,.54);
}
input:checked + section.box .box-title:before {
  transform: rotate(90deg);
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: center;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white; /* White text */
  padding: 5px 12px; /* Some padding */
  cursor: pointer; /* Pointer/hand icon */
  float: center; /* Float the buttons side by side */
}

#toc_container {
  float: left;
  width: auto;
  background: #eee;
  font-size: 0.8em;
  padding: 1em 1em;
  margin: 0 1em 0 1em;
  border: 1px solid #aaa;
}

.toc_title {
    font-weight: 700;
    text-align: center;
}

#toc_container ul>li {
margin: 0 0 .6em 0;
list-style-type: none;
}

#toc_container a {
text-decoration: none;
color: #3D5AFE;
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}

#toc_scroll {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  border: 2px solid #008CBA;
  outline: none;
  background-color: white;
  color: black;
  cursor: pointer;
  padding: 5px;
  font-size: 18px;
  border-radius: 8px;
}

#toc_scroll:hover {
  background-color: #aaa;
}

</style>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<!--<script src="lib/mobile-detect.min.js"></script>-->
<script src="lib/template.v1.js"></script>


<script type="text/front-matter">
  title: "LSDs"
  description: ""
</script>
<body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/lsds_header.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">Direction vectors of the
    LSDs on a single neuron from the fly visual system. Colors correspond to
    the direction in which the neuronal processes travel.</figcaption></td>
  </tr></table>

</div>

<dt-article class="centered" id="dtbody">

<dt-byline class="l-page transparent"></dt-byline>
<h1>Local Shape Descriptors for Neuron Segmentation</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=Md__FUQAAAAJ&hl=en&oi=ao">Arlo Sheridan</a>
        <a class="affiliation" href="https://www.salk.edu/science/core-facilities/advanced-biophotonics">Salk Institute</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=AnU408sAAAAJ&hl=en">Tri M. Nguyen</a>
        <a class="affiliation" href="https://www.lee.hms.harvard.edu/">Harvard University</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=gBcB_UUAAAAJ&hl=en">Diptodip Deb</a>
        <a class="affiliation" href="https://www.janelia.org/lab/turaga-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=y2s07ssAAAAJ&hl=en">Wei-Chung Allen Lee</a>
        <a class="affiliation" href="https://www.lee.hms.harvard.edu/">Harvard University</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=oSGyzt4AAAAJ&hl=en">Stephan Saalfeld</a>
        <a class="affiliation" href="https://www.janelia.org/lab/saalfeld-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=V_NdI3sAAAAJ&hl=en">Srini Turaga</a>
        <a class="affiliation" href="https://www.janelia.org/lab/turaga-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=_ZLy6tEAAAAJ&hl=en">Uri Manor</a>
        <a class="affiliation" href="https://www.salk.edu/science/core-facilities/advanced-biophotonics">Salk Institute</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=7rqAapgAAAAJ&hl=en">Jan Funke</a>
        <a class="affiliation" href="https://www.janelia.org/lab/funke-lab">HHMI Janelia</a>
    </div>
  </div>
  <div class="date">
    <div class="month">LSD</div>
    <div class="year" style="color: #2AAFCF;"><a href="https://github.com/funkelab/lsd_paper" target="_blank">Paper</a></div>
  </div>
</div>
</dt-byline>
</dt-byline>

<button onclick="topFunction()" id="toc_scroll" title="Scroll to toc">Contents</button>

<script>
//Get the button
var mybutton = document.getElementById("toc_scroll");

// When the user scrolls down 2000px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  var toc = document.getElementById("toc_container");
  if (window.scrollY > (toc.offsetTop + toc.offsetHeight)) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.getElementById("toc_container").scrollIntoView();
}

function magnify(imgID, zoom, secondim) {
  var img, glass, w, h, bw, simg;
  img = document.getElementById(imgID);
  simg = document.getElementById(secondim);
  /*create magnifier glass:*/
  glass = document.createElement("DIV");
  glass.setAttribute("class", "img-magnifier-glass");
  /*insert magnifier glass:*/
  img.parentElement.insertBefore(glass, img);
  /*set background properties for the magnifier glass:*/
  glass.style.backgroundImage = "url('" + simg.src + "')";
  glass.style.backgroundRepeat = "no-repeat";
  glass.style.backgroundSize = (img.width * zoom) + "px " + (img.height * zoom) + "px";
  bw = 3;
  w = glass.offsetWidth / 2;
  h = glass.offsetHeight / 2;
  /*execute a function when someone moves the magnifier glass over the image:*/
  glass.addEventListener("mousemove", moveMagnifier);
  img.addEventListener("mousemove", moveMagnifier);
  /*and also for touch screens:*/
  glass.addEventListener("touchmove", moveMagnifier);
  img.addEventListener("touchmove", moveMagnifier);
  function moveMagnifier(e) {
    var pos, x, y;
    /*prevent any other actions that may occur when moving over the image*/
    e.preventDefault();
    /*get the cursor's x and y positions:*/
    pos = getCursorPos(e);
    x = pos.x;
    y = pos.y;
    /*prevent the magnifier glass from being positioned outside the image:*/
    if (x > img.width - (w / zoom)) {x = img.width - (w / zoom);}
    if (x < w / zoom) {x = w / zoom;}
    if (y > img.height - (h / zoom)) {y = img.height - (h / zoom);}
    if (y < h / zoom) {y = h / zoom;}
    /*set the position of the magnifier glass:*/
    glass.style.left = (x - w) + "px";
    glass.style.top = (y - h) + "px";
    /*display what the magnifier glass "sees":*/
    glass.style.backgroundPosition = "-" + ((x * zoom) - w + bw) + "px -" + ((y * zoom) - h + bw) + "px";
  }
  function getCursorPos(e) {
    var a, x = 0, y = 0;
    e = e || window.event;
    /*get the x and y positions of the image:*/
    a = img.getBoundingClientRect();
    /*calculate the cursor's x and y coordinates, relative to the image:*/
    x = e.pageX - a.left;
    y = e.pageY - a.top;
    /*consider any page scrolling:*/
    x = x - window.pageXOffset;
    y = y - window.pageYOffset;
    return {x : x, y : y};
  }
}

</script>
<h2>Abstract</h2>
<p>We present a novel approach to neuron segmentation in electron microscopy
volumes, which is based on the prediction of Local Shape Descriptors (LSDs) in
combination with conventional voxel-wise direct neighbor affinities for neuron
boundary detection. The shape descriptors capture local statistics about the
neuron to be segmented, such as diameter, elongation, and direction. We show
that learning to predict LSDs as an auxiliary learning task significantly
improves the accuracy of the predicted affinities and subsequently of the
obtained segmentations. We compare our method in a large comparative study
against several existing methods across various specimen, imaging techniques,
and resolutions. The results demonstrate that auxiliary learning of LSDs
consistently increases segmentation accuracy over a range of metrics on all
investigated datasets, when compared to other affinity-based methods. Notably,
our method is competitive with the current state of the art for neuron
segmentation, albeit two orders of magnitude more efficient—a critical
requirement for the processing of future petabyte-sized datasets. Our method,
together with evaluation code and consolidated evaluation datasets, is publicly
available as a benchmark for future method contributions.</p>
<hr>
<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
  <li><a href="#background">1. Background</a>
  <ul>
    <li><a href="#connectomics">1.1 Connectomics</a></li>
    <li><a href="#neuron_segmentation">1.2 Neuron Segmentation</a></li>
    <li><a href="#related_work">1.3 Related Work</a></li>
    <li><a href="#contributions">1.4 Contributions</a></li>
  </ul>
</li>
<li><a href="#methods">2. Methods</a></li>
  <ul>
    <li><a href="#local_shape_descriptors">2.1 Local Shape Descriptors</a></li>
    <li><a href="#network_architectures">2.2 Network Architectures</a></li>
  </ul>
<li><a href="#results">3. Results</a></li>
  <ul>
    <li><a href="#networks">3.1 Networks</a></li>
    <li><a href="#metrics">3.2 Metrics</a></li>
    <li><a href="#datasets">3.3 Datasets</a></li>
    <li><a href="#accuracy">3.4 Accuracy</a></li>
    <li><a href="#throughput">3.5 Throughput</a></li>
  </ul>
<li><a href="#discussion">4. Discussion</a></li>
  <ul>
    <li><a href="#metric_eval">4.1 Metric Evaluation</a></li>
    <li><a href="#auxiliary_learning">4.2 Auxiliary Learning</a></li>
    <li><a href="#auto_context">4.3 Auto-Context</a></li>
    <li><a href="#masking">4.4 Masking</a></li>
  </ul>
<li><a href="#conclusions">5. Conclusions</a></li>
<li><a href="#tldr">6. Tl;dr</a></li>
<li><a href="#acknowledgements">7. Acknowledgements</a></li>
<li><a href="#code">8. Code</a></li>
<li><a href="#supplementary">9. Supplementary</a></li>
<li><a href="#references">10. References</a></li>
</ul>
</div>
<h2 id="background">Background</h2>
<h3 id="connectomics">Connectomics</h3>
<p>Connectomics is an emerging field which integrates multiple domains including
neuroscience, microscopy, and computer science. The overarching goal is to
provide insights about the brain at resolutions which are not achievable with
other approaches. The ability to study neural structures at this scale will
hopefully lead to a better understanding of brain disorders, and subsequently
advance medical approaches towards finding treatments &amp; cures.</p>
<p>The basic idea is to produce &quot;connectomes&quot; which are essentially maps of the
brain. These maps, or &quot;wiring diagrams&quot;, give scientists the ability to see how
every neuron interacts through synaptic connections. They can be used to
complement existing techniques <dt-cite
key="schlegel_synaptic_2016,turner-evans_neuroanatomical_2020"></dt-cite> and
drive future experiments <dt-cite
key="schneider-mizell_quantitative_2016,motta_dense_2019,bates_complete_2020"></dt-cite>.</p>
<p>Okay, but how are the brain maps generated?</p>
<p>Before generating neural wiring diagrams, we first need to acquire the brain
tissue to use. Currently, only Electron Microscopy (EM) allows imaging of neural
tissue at a resolution sufficient to see individual synapses. After extracting a
brain (for example, from a fruit fly), the tissue is generally stained with
heavy metals to increase contrast between structures of interest (i.e neuron
membranes). Once stained, the tissue is imaged with an electron microscope.
There are a few types of EM imaging approaches; the most common being ssTEM and
FIB-SEM. The former method involves slicing the brain into super thin (e.g 40
nanometer) sections. The latter uses an ion beam to erode the tissue. In either
case, electrons are shot at the tissue to produce an image of the data. This is
a way oversimplified explanation, for a better overview of imaging techniques,
see <dt-cite key="briggman_volume_2012"> this paper</dt-cite>, specifically
Figure 1.</p>
<p>Sweet! Let's image a human brain and be done with it.</p>
<p>Unfortunately, by imaging brains at such high resolution, the resulting data is
massive. Let's consider the fruit fly example.  A full adult fruit fly brain
(<strong>FAFB</strong>) imaged with ssTEM <dt-cite key="zheng_complete_2018"></dt-cite> at a
pixel resolution of ~4 nanometers and ~40 nanometer thick sections, comprises
~213 teravoxels of data (~50 teravoxels of actual brain tissue)<dt-cite
key="heinrich_synaptic_2018"></dt-cite>. For reference, a voxel is a volumetric
pixel, and the &quot;tera&quot; prefix means 10<sup>12</sup>. So, one fly brain contains
upwards of 213,000,000,000,000 volumetric pixels. To put that in perspective,
<dt-cite key="abbott_mind_2020">Abbott et al.</dt-cite> argue that, assuming a
scale where 1000 cubic microns is equivalent to 1 centimeter, a fruit fly brain
would comprise the length of 6 and a half Boeing 747 aeroplanes. This still
pales in comparison to a mouse brain which would be the distance from Boston to
Lisbon, and require the acquisition of 1 million terabytes of data.</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/scale1.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">Scale perspective. A
fruit fly brain imaged at synaptic resolution takes up 100's of terabytes of
storage space. It allows us to see fine structures such as neural plasma
membranes (pink arrow), synapses (blue arrow), vesicles (green arrow) and
mitochondria (orange arrow). 3D fruit fly model kindly provided by <a
href="https://scholar.google.com/citations?user=ir1vhA8AAAAJ&hl=en"
target="_blank">Igor Siwanowicz</a></figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<p>Try navigating the fly brain in an interactive <a
href="https://github.com/google/neuroglancer" target="_blank">Neuroglancer</a>
viewer (click question mark for controls). Think, Google Earth for brains:</p>
<html>
  <body>
    <div class="accordion-container">
      <nav class="accordion arrows">
        <input type="radio" name="accordion" id="cb1" checked/>
        <section class="box">
          <label class="box-title" for="cb1">Full adult fly brain interactive viewer</label>
          <label class="box-close" for="acc-open"></label>
          <input type="radio" name="accordion" id="acc-open"/>
          <div class="box-content">
            <div class="responsive-container">
              <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%2C%22position%22:%5B63696.5703125%2C30152.005859375%2C3242.8369140625%5D%2C%22crossSectionScale%22:413.08059520030787%2C%22projectionOrientation%22:%5B-0.06840167939662933%2C-0.4631274342536926%2C-0.204045370221138%2C0.8597671985626221%5D%2C%22projectionScale%22:124585.76409043159%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-fafb-data/fafb_v14/fafb_v14_clahe%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#0088ff%22%2C%22name%22:%22fafb_v14_clahe%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22fafb_v14_clahe%22%7D%2C%22layout%22:%224panel%22%2C%22partialViewport%22:%5B0%2C0%2C1%2C1%5D%7D" "></iframe>
            </div>
          </div>
        </section>
      </nav>
    </div>
  </body>
</html>
<p>Okay, now we have the data, so how do we create the wiring diagrams?</p>
<p>To create a wiring diagram, we need to reconstruct all of the neurons and their
synaptic connections. This process can be done manually - which consists of
human annotators navigating these datasets and labeling every neuron and their
synaptic partners using various software <dt-cite
key="saalfeld2009catmaid,boergens_webknossos_2017,berger_vast_2018,zhao_neutu_2018"></dt-cite>.
However, this can become extremely tedious and expensive (<strong>$$$</strong>) given the
size of the datasets. For example, simply reconstructing 129 neurons from
<strong>FAFB</strong> took a team of tracers ~60 days to complete<dt-cite
key="zheng_complete_2018"></dt-cite>. Given that a fruit fly has ~100,000
neurons, purely manual reconstruction of connectomes is obviously infeasible.</p>
<p>Consequently, methods have been developed to automate this process. From here
on, we will focus on the automatic reconstruction of neurons. To see the current
approaches to synapse detection, check <dt-cite
key="kreshuk_who_2015,heinrich_synaptic_2018,huang_fully-automatic_2018,buhmann_automatic_2020">
these papers</dt-cite> out!</p>
<h3 id="neuron_segmentation">Neuron Segmentation</h3>
<ul>
<li>why is neuron segmentation hard</li>
<li>describe what neuron segmentation is</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/detection_vs_segmentation_shoes.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/detection_vs_segmentation_neurons.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<h3 id="related_work">Related Work</h3>
<ul>
<li>boundaries</li>
<li>affinities</li>
<li>long range</li>
<li>malis</li>
<li>ffn</li>
<li>metric learning</li>
<li>watershed / agglom variants</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/related_methods.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<h3 id="contributions">Contributions</h3>
<ul>
<li>introduce lsds</li>
<li>large scale study</li>
</ul>
<hr>
<h2 id="methods">Methods</h2>
<h3 id="local_shape_descriptors">Local Shape Descriptors</h3>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/lsd_schematic.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>todo: add interactive lsd overview</li>
</ul>
<!--<div class="row"> -->
  <!--<div class="column">-->
  <!--<div class="a">-->
  <!--<img id="raw1" src="/assets/img/raw.png">-->
  <!--</div>-->
  <!--<div class="b">-->
  <!--<img id="offsets" src="/assets/img/offsets.png">-->
  <!--</div>-->
  <!--</div>-->
  <!--<div class="column">-->
  <!--<div class="a">-->
  <!--<img id="raw2" src="/assets/img/raw.png">-->
  <!--</div>-->
  <!--<div class="b">-->
  <!--<img id="orthogs" src="/assets/img/orthogs.png">-->
  <!--</div>-->
  <!--</div>-->
  <!--<div class="column">-->
  <!--<div class="a">-->
  <!--<img id="raw3" src="/assets/img/raw.png">-->
  <!--</div>-->
  <!--<div class="b">-->
  <!--<img id="size" src="/assets/img/size.png">-->
  <!--</div>-->
  <!--</div>-->
<!--</div>-->
<!--<script>-->
<!--magnify("raw1", 1, "offsets");-->
<!--magnify("raw2", 1, "orthogs");-->
<!--magnify("raw3", 1, "size");-->
<!--</script>-->
<h3 id="network_architectures">Network Architectures</h3>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="cb2" />
  <section class="box">
  <label class="box-title" for="cb2">test</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div>
    <p> explain lsd architecture </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img src="assets/img/lsd.png" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
    <p> explain mtlsd architecture </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img src="assets/img/mtlsd.png" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
    <p> explain aclsd architecture </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img src="assets/img/aclsd.png" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
    <p> explain acrlsd architecture </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img src="assets/img/acrlsd.png" style="display: block; margin: auto; width: 100%;"/>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<hr>
<h2 id="results">Results</h2>
<h3 id="networks">Networks</h3>
<h3 id="metrics">Metrics</h3>
<h3 id="datasets">Datasets</h3>
<h3 id="accuracy">Accuracy</h3>
<p>blah blah blah blah
blah blah blah blah
blah blah blah blah
blah blah blah blah
blah blah blah blah</p>
<div class="plot-container">
  <iframe
    class="responsive-iframe"
    src="../plots/test.html"
    sandbox="allow-same-origin allow-scripts"
    scrolling="no"
    seamless="seamless"
    frameborder="0">
  </iframe>
</div>
<p>blah blah blah blah
blah blah blah blah
blah blah blah blah
blah blah blah blah
blah blah blah blah
blah blah blah blah</p>
<h3 id="throughput">Throughput</h3>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="block" />
  <section class="box">
  <label class="box-title" for="block">test</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div>
    <p> blah blah blah </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img class="b-lazy"
    src="assets/videos/watershed_separate_joined.gif" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
    <p> blah blah blah </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img class="b-lazy"
    src="assets/videos/agglom_separate_joined.gif" style="display: block; margin: auto; width: 100%;"/>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<hr>
<h2 id="discussion">Discussion</h2>
<h3 id="metric_eval">Metric Evaluation</h3>
<h3 id="auxiliary_learning">Auxiliary Learning</h3>
<h3 id="auto_context">Auto-Context</h3>
<h3 id="masking">Masking</h3>
<hr>
<h2 id="conclusions">Conclusions</h2>
<hr>
<h2 id="tldr">Tl;dr</h2>
<ul>
<li>
<p>Connectomics is a relatively new field combining neuroscience, microscopy, biology and computer science.</p>
</li>
<li>
<p>The goal is to generate maps of the brain at synaptic resolution. By doing so,
it will hopefully lead to a better understanding of how things work and
subsequently advance medical approaches to various diseases.</p>
</li>
<li>
<p>The datasets required to produce these brain maps are massive since they have
to be imaged at such a high resolution.</p>
</li>
<li>
<p>Manually reconstructing wiring diagrams in the datasets is extremely time consuming and
expensive so there is a great need to automate the process.</p>
</li>
<li>
<p>Reconstructing neurons is challenging because many consecutively correct decisions must be made. Errors can propagate throughout a dataset easily.</p>
</li>
<li>
<p>Methods need to also be computationally efficient and scalable to account
for the size of the data.</p>
</li>
<li>
<p>We present a novel approach to neuron segmentation, Local Shape Descriptors
(LSDs) - a 10-Dimensional embedding used as an auxiliary learning objective
for boundary detection.</p>
</li>
<li>
<p>We find that the LSDs help improve boundaries and subsequent neuron
reconstructions in several large and diverse datasets.</p>
</li>
<li>
<p>They are also two orders of magnitude faster than the current state of the art
approach.</p>
</li>
</ul>
</dt-article>
<dt-appendix>
<h2 id="acknowledgements">Acknowledgements</h2>
<hr>
<h2 id="code">Code</h2>
<hr>
<h2 id="supplementary">Supplementary</h2>
<hr>
<h2 id="references"></h2>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@article{januszewski_high-precision_2018,
	title = {High-precision automated reconstruction of neurons with flood-filling networks},
	volume = {15},
	issn = {1548-7105},
	doi = {10.1038/s41592-018-0049-4},
	number = {8},
	journal = {Nature Methods},
	author = {Januszewski, Michał and Kornfeld, Jörgen and Li, Peter H. and Pope, Art and Blakely, Tim and Lindsey, Larry and Maitin-Shepard, Jeremy and Tyka, Mike and Denk, Winfried and Jain, Viren},
	year = {2018},
	pages = {605},
        url = {https://www.nature.com/articles/s41592-018-0049-4}
}
@article{zheng_complete_2018,
	title = {A Complete Electron Microscopy Volume of the Brain of Adult Drosophila melanogaster},
	volume = {174},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2018.06.019},
	number = {3},
	journal = {Cell},
	author = {Zheng, Zhihao and Lauritzen, J. Scott and Perlman, Eric and Robinson, Camenzind G. and Nichols, Matthew and Milkie, Daniel and Torrens, Omar and Price, John and Fisher, Corey B. and Sharifi, Nadiya and Calle-Schuler, Steven A. and Kmecova, Lucia and Ali, Iqbal J. and Karsh, Bill and Trautman, Eric T. and Bogovic, John A. and Hanslovsky, Philipp and Jefferis, Gregory S. X. E. and Kazhdan, Michael and Khairy, Khaled and Saalfeld, Stephan and Fetter, Richard D. and Bock, Davi D.},
	year = {2018},
	pages = {730--743},
        url = {https://www.sciencedirect.com/science/article/pii/S0092867418307876}
}
@article{lee_superhuman_2017,
	title = {Superhuman {Accuracy} on the {SNEMI}3D {Connectomics} {Challenge}},
	url = {http://arxiv.org/abs/1706.00120},
	journal = {arXiv:1706.00120 [cs]},
	author = {Lee, Kisuk and Zung, Jonathan and Li, Peter and Jain, Viren and Seung, H. Sebastian},
	year = {2017},
}
@article{luther_learning_2019,
	title = {Learning {Metric} {Graphs} for {Neuron} {Segmentation} {In} {Electron} {Microscopy} {Images}},
	journal = {arXiv:1902.00100 [cs]},
	author = {Luther, Kyle and Seung, H. Sebastian},
	year = {2019},
}
@inproceedings{ronneberger_u-net:_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	doi = {10.1007/978-3-319-24574-4_28},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	pages = {234--241}
}
@article{turaga_maximin_2009,
	title = {Maximin affinity learning of image segmentation},
	url = {http://arxiv.org/abs/0911.5372},
	urldate = {2020-10-05},
	journal = {arXiv:0911.5372 [cs]},
	author = {Turaga, Srinivas C. and Briggman, Kevin L. and Helmstaedter, Moritz and Denk, Winfried and Seung, H. Sebastian},
	month = nov,
	year = {2009},
	note = {arXiv: 0911.5372},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}
@article{beier2017multicut,
  title={Multicut brings automated neurite segmentation closer to human performance},
  author={Beier, Thorsten and Pape, Constantin and Rahaman, Nasim and Prange, Timo and Berg, Stuart and Bock, Davi D and Cardona, Albert and Knott, Graham W and Plaza, Stephen M and Scheffer, Louis K and others},
  journal={Nature methods},
  volume={14},
  number={2},
  pages={101--102},
  year={2017},
  publisher={Nature Publishing Group}
}
@inproceedings{cicek_3d_2016,
	title = {3D {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2016},
	author = {\c{C}i\c{c}ek, \"{O}zg\"{u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	year = {2016},
	pages = {424--432},
}
@article{funke_large_2019,
	title = {Large {Scale} {Image} {Segmentation} with {Structured} {Loss} {Based} {Deep} {Learning} for {Connectome} {Reconstruction}},
	volume = {41},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Funke, Jan and Tschopp, Fabian and Grisaitis, William and Sheridan, Arlo and Singh, Chandan and Saalfeld, Stephan and Turaga, Srinivas C.},
	year = {2019},
	pages = {1669--1680},
}
@incollection{ciresan_deep_2012,
	title = {Deep {Neural} {Networks} {Segment} {Neuronal} {Membranes} in {Electron} {Microscopy} {Images}},
	url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
	urldate = {2020-06-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, Jürgen},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2843--2851}
}
@article{turaga_convolutional_2010,
	title = {Convolutional {Networks} {Can} {Learn} to {Generate} {Affinity} {Graphs} for {Image} {Segmentation}},
	volume = {22},
	issn = {0899-7667},
	doi = {10.1162/neco.2009.10-08-881},
	number = {2},
	journal = {Neural Computation},
	author = {Turaga, Srinivas C. and Murray, Joseph F. and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H. Sebastian},
	month = feb,
	year = {2010},
	note = {Conference Name: Neural Computation},
	pages = {511--538}
}
@inproceedings{bai_deep_2017,
	title = {Deep {Watershed} {Transform} for {Instance} {Segmentation}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Bai_Deep_Watershed_Transform_CVPR_2017_paper.html},
	urldate = {2020-06-30},
	author = {Bai, Min and Urtasun, Raquel},
	year = {2017},
	pages = {5221--5229}
}
@article{takemura_synaptic_2015,
	title = {Synaptic circuits and their variations within different columns in the visual system of {Drosophila}},
	volume = {112},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640747/},
	doi = {10.1073/pnas.1509820112},
	number = {44},
	urldate = {2020-07-22},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Takemura, Shin-ya and Xu, C. Shan and Lu, Zhiyuan and Rivlin, Patricia K. and Parag, Toufiq and Olbris, Donald J. and Plaza, Stephen and Zhao, Ting and Katz, William T. and Umayam, Lowell and Weaver, Charlotte and Hess, Harald F. and Horne, Jane Anne and Nunez-Iglesias, Juan and Aniceto, Roxanne and Chang, Lei-Ann and Lauchie, Shirley and Nasca, Ashley and Ogundeyi, Omotara and Sigmund, Christopher and Takemura, Satoko and Tran, Julie and Langille, Carlie and Le Lacheur, Kelsey and McLin, Sari and Shinomiya, Aya and Chklovskii, Dmitri B. and Meinertzhagen, Ian A. and Scheffer, Louis K.},
	month = nov,
	year = {2015},
	pmid = {26483464},
	pmcid = {PMC4640747},
	pages = {13711--13716},
}
@article{lee_learning_2019,
	title = {Learning {Dense} {Voxel} {Embeddings} for {3D} {Neuron} {Reconstruction}},
	url = {http://arxiv.org/abs/1909.09872},
	urldate = {2020-09-02},
	journal = {arXiv:1909.09872 [cs]},
	author = {Lee, Kisuk and Lu, Ran and Luther, Kyle and Seung, H. Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.09872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{tu_auto-context_2010,
	title = {Auto-{Context} and {Its} {Application} to {High}-{Level} {Vision} {Tasks} and {3D} {Brain} {Image} {Segmentation}},
	volume = {32},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2009.186},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tu, Zhuowen and Bai, Xiang},
	month = oct,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1744--1757}
}
@article{meila_comparing_2007,
	title = {Comparing clusterings—an information based distance},
	volume = {98},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X06002016},
	doi = {10.1016/j.jmva.2006.11.013},
	language = {en},
	number = {5},
	urldate = {2020-09-10},
	journal = {Journal of Multivariate Analysis},
	author = {Meilă, Marina},
	month = may,
	year = {2007},
	keywords = {Agreement measures, Clustering, Comparing partitions, Information theory, Mutual information, Similarity measures},
	pages = {873--895}
}
@incollection{ferrari_mutex_2018,
	address = {Cham},
	title = {The {Mutex} {Watershed}: {Efficient}, {Parameter}-{Free} {Image} {Partitioning}},
	volume = {11208},
	isbn = {978-3-030-01224-3 978-3-030-01225-0},
	shorttitle = {The {Mutex} {Watershed}},
	url = {http://link.springer.com/10.1007/978-3-030-01225-0_34},
	language = {en},
	urldate = {2020-09-15},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wolf, Steffen and Pape, Constantin and Bailoni, Alberto and Rahaman, Nasim and Kreshuk, Anna and Köthe, Ullrich and Hamprecht, Fred A.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01225-0_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {571--587}
}
@article{li_automated_2019,
	title = {Automated {Reconstruction} of a {Serial}-{Section} {EM} {Drosophila} {Brain} with {Flood}-{Filling} {Networks} and {Local} {Realignment}},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/605634v1},
	doi = {10.1101/605634},
	language = {en},
	urldate = {2020-09-16},
	journal = {bioRxiv},
	author = {Li, Peter H. and Lindsey, Larry F. and Januszewski, Michał and Zheng, Zhihao and Bates, Alexander Shakeel and Taisz, István and Tyka, Mike and Nichols, Matthew and Li, Feng and Perlman, Eric and Maitin-Shepard, Jeremy and Blakely, Tim and Leavitt, Laramie and Jefferis, Gregory S. X. E. and Bock, Davi and Jain, Viren},
	month = apr,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {605634}
}
@article{abbott_mind_2020,
	title = {The {Mind} of a {Mouse}},
	volume = {182},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867420310011},
	doi = {10.1016/j.cell.2020.08.010},
	language = {en},
	number = {6},
	urldate = {2020-09-17},
	journal = {Cell},
	author = {Abbott, Larry F. and Bock, Davi D. and Callaway, Edward M. and Denk, Winfried and Dulac, Catherine and Fairhall, Adrienne L. and Fiete, Ila and Harris, Kristen M. and Helmstaedter, Moritz and Jain, Viren and Kasthuri, Narayanan and LeCun, Yann and Lichtman, Jeff W. and Littlewood, Peter B. and Luo, Liqun and Maunsell, John H. R. and Reid, R. Clay and Rosen, Bruce R. and Rubin, Gerald M. and Sejnowski, Terrence J. and Seung, H. Sebastian and Svoboda, Karel and Tank, David W. and Tsao, Doris and Van Essen, David C.},
	month = sep,
	year = {2020},
	pages = {1372--1376}
}
@article{turner-evans_neuroanatomical_2020,
	title = {The {Neuroanatomical} {Ultrastructure} and {Function} of a {Biological} {Ring} {Attractor}},
	volume = {0},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(20)30613-9},
	doi = {10.1016/j.neuron.2020.08.006},
	language = {English},
	number = {0},
	urldate = {2020-09-17},
	journal = {Neuron},
	author = {Turner-Evans, Daniel B. and Jensen, Kristopher T. and Ali, Saba and Paterson, Tyler and Sheridan, Arlo and Ray, Robert P. and Wolff, Tanya and Lauritzen, J. Scott and Rubin, Gerald M. and Bock, Davi D. and Jayaraman, Vivek},
	month = sep,
	year = {2020},
	pmid = {32916090},
	note = {Publisher: Elsevier},
	keywords = {behavior, central complex, Drosophila, electron microscopy, head direction, navigation, network dynamics, neural circuit, RNA-seq, two-photon calcium imaging}
}
@article{bates_complete_2020,
	title = {Complete {Connectomic} {Reconstruction} of {Olfactory} {Projection} {Neurons} in the {Fly} {Brain}},
	volume = {30},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982220308587},
	doi = {10.1016/j.cub.2020.06.042},
	language = {en},
	number = {16},
	urldate = {2020-09-17},
	journal = {Current Biology},
	author = {Bates, Alexander S. and Schlegel, Philipp and Roberts, Ruairi J. V. and Drummond, Nikolas and Tamimi, Imaan F. M. and Turnbull, Robert and Zhao, Xincheng and Marin, Elizabeth C. and Popovici, Patricia D. and Dhawan, Serene and Jamasb, Arian and Javier, Alexandre and Serratosa Capdevila, Laia and Li, Feng and Rubin, Gerald M. and Waddell, Scott and Bock, Davi D. and Costa, Marta and Jefferis, Gregory S. X. E.},
	month = aug,
        year = {2020},
	keywords = {connectomics, Drosophila, EM, memory, neuroanatomy, olfaction, synapses},
	pages = {3183--3199.e6}
}
@article{motta_dense_2019,
	title = {Dense connectomic reconstruction in layer 4 of the somatosensory cortex},
	volume = {366},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/366/6469/eaay3134},
	doi = {10.1126/science.aay3134},
	number = {6469},
	journal = {Science},
	author = {Motta, Alessandro and Berning, Manuel and Boergens, Kevin M. and Staffler, Benedikt and Beining, Marcel and Loomba, Sahil and Hennig, Philipp and Wissler, Heiko and Helmstaedter, Moritz},
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/366/6469/eaay3134.full.pdf}
}
@article{schlegel_synaptic_2016,
	title = {Synaptic transmission parallels neuromodulation in a central food-intake circuit},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.16799},
	doi = {10.7554/eLife.16799},
	urldate = {2020-09-17},
	journal = {eLife},
	author = {Schlegel, Philipp and Texada, Michael J and Miroschnikow, Anton and Schoofs, Andreas and Hückesfeld, Sebastian and Peters, Marc and Schneider-Mizell, Casey M and Lacin, Haluk and Li, Feng and Fetter, Richard D and Truman, James W and Cardona, Albert and Pankratz, Michael J},
	editor = {Calabrese, Ronald L},
	month = nov,
	year = {2016},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {acetylcholine, co-transmission, endocrine, hugin, neuromedin, neuropeptides},
	pages = {e16799}
}
@article{schneider-mizell_quantitative_2016,
	title = {Quantitative neuroanatomy for connectomics in {Drosophila}},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.12059},
	doi = {10.7554/eLife.12059},
	urldate = {2020-09-17},
	journal = {eLife},
	author = {Schneider-Mizell, Casey M and Gerhard, Stephan and Longair, Mark and Kazimiers, Tom and Li, Feng and Zwart, Maarten F and Champion, Andrew and Midgley, Frank M and Fetter, Richard D and Saalfeld, Stephan and Cardona, Albert},
	editor = {Calabrese, Ronald L},
	month = mar,
	year = {2016},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {connectomics, neuroanatomy, proprioception},
	pages = {e12059}
}
@article{briggman_volume_2012,
	series = {Neurotechnology},
	title = {Volume electron microscopy for neuronal circuit reconstruction},
	volume = {22},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438811001887},
	doi = {10.1016/j.conb.2011.10.022},
	language = {en},
	number = {1},
	urldate = {2020-09-17},
	journal = {Current Opinion in Neurobiology},
	author = {Briggman, Kevin L and Bock, Davi D},
	month = feb,
	year = {2012},
	pages = {154--161}
}
@article{dorkenwald_binary_2019,
	title = {Binary and analog variation of synapses between cortical pyramidal neurons},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2019.12.29.890319v1},
	doi = {10.1101/2019.12.29.890319},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Dorkenwald, Sven and Turner, Nicholas L. and Macrina, Thomas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Bodor, Agnes L. and Bleckert, Adam A. and Brittain, Derrick and Kemnitz, Nico and Silversmith, William M. and Ih, Dodam and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Yu, Szi-Chieh and Popovych, Sergiy and Wong, William and Castro, Manuel and Jordan, Chris S. and Wilson, Alyssa M. and Froudarakis, Emmanouil and Buchanan, JoAnn and Takeno, Marc and Torres, Russel and Mahalingam, Gayathri and Collman, Forrest and Schneider-Mizell, Casey and Bumbarger, Daniel J. and Li, Yang and Becker, Lynne and Suckow, Shelby and Reimer, Jacob and Tolias, Andreas S. and Costa, Nuno Maçarico da and Reid, R. Clay and Seung, H. Sebastian},
	month = dec,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {2019.12.29.890319}
}
@article{maniates-selvin_reconstruction_2020,
	title = {Reconstruction of motor control circuits in adult {Drosophila} using automated transmission electron microscopy},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.01.10.902478v1},
	doi = {10.1101/2020.01.10.902478},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Maniates-Selvin, Jasper T. and Hildebrand, David Grant Colburn and Graham, Brett J. and Kuan, Aaron T. and Thomas, Logan A. and Nguyen, Tri and Buhmann, Julia and Azevedo, Anthony W. and Shanny, Brendan L. and Funke, Jan and Tuthill, John C. and Lee, Wei-Chung Allen},
	month = jan,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {2020.01.10.902478}
}
@article{heinrich_synaptic_2018,
	title = {Synaptic {Cleft} {Segmentation} in {Non}-{Isotropic} {Volume} {Electron} {Microscopy} of the {Complete} {Drosophila} {Brain}},
	url = {http://arxiv.org/abs/1805.02718},
	urldate = {2020-09-17},
	journal = {arXiv:1805.02718 [cs]},
	author = {Heinrich, Larissa and Funke, Jan and Pape, Constantin and Nunez-Iglesias, Juan and Saalfeld, Stephan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.02718},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{scheffer_connectome_2020,
	title = {A {Connectome} and {Analysis} of the {Adult} {Drosophila} {Central} {Brain}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.07.030213v1},
	doi = {10.1101/2020.04.07.030213},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Scheffer, Louis K. and Xu, C. Shan and Januszewski, Michal and Lu, Zhiyuan and Takemura, Shin-ya and Hayworth, Kenneth J. and Huang, Gary B. and Shinomiya, Kazunori and Maitin-Shepard, Jeremy and Berg, Stuart and Clements, Jody and Hubbard, Philip and Katz, William and Umayam, Lowell and Zhao, Ting and Ackerman, David and Blakely, Tim and Bogovic, John and Dolafi, Tom and Kainmueller, Dagmar and Kawase, Takashi and Khairy, Khaled A. and Leavitt, Laramie and Li, Peter H. and Lindsey, Larry and Neubarth, Nicole and Olbris, Donald J. and Otsuna, Hideo and Trautman, Eric T. and Ito, Masayoshi and Goldammer, Jens and Wolff, Tanya and Svirskas, Robert and Schlegel, Philipp and Neace, Erika R. and Knecht, Christopher J. and Alvarado, Chelsea X. and Bailey, Dennis A. and Ballinger, Samantha and Borycz, Jolanta A. and Canino, Brandon S. and Cheatham, Natasha and Cook, Michael and Dreher, Marisa and Duclos, Octave and Eubanks, Bryon and Fairbanks, Kelli and Finley, Samantha and Forknall, Nora and Francis, Audrey and Hopkins, Gary Patrick and Joyce, Emily M. and Kim, SungJin and Kirk, Nicole A. and Kovalyak, Julie and Lauchie, Shirley A. and Lohff, Alanna and Maldonado, Charli and Manley, Emily A. and McLin, Sari and Mooney, Caroline and Ndama, Miatta and Ogundeyi, Omotara and Okeoma, Nneoma and Ordish, Christopher and Padilla, Nicholas and Patrick, Christopher and Paterson, Tyler and Phillips, Elliott E. and Phillips, Emily M. and Rampally, Neha and Ribeiro, Caitlin and Robertson, Madelaine K. and Rymer, Jon Thomson and Ryan, Sean M. and Sammons, Megan and Scott, Anne K. and Scott, Ashley L. and Shinomiya, Aya and Smith, Claire and Smith, Kelsey and Smith, Natalie L. and Sobeski, Margaret A. and Suleiman, Alia and Swift, Jackie and Takemura, Satoko and Talebi, Iris and Tarnogorska, Dorota and Tenshaw, Emily and Tokhi, Temour and Walsh, John J. and Yang, Tansy and Horne, Jane Anne and Li, Feng and Parekh, Ruchi and Rivlin, Patricia K. and Jayaraman, Vivek and Ito, Kei and Saalfeld, Stephan and George, Reed and Meinertzhagen, Ian A. and Rubin, Gerald M. and Hess, Harald F. and Jain, Viren and Plaza, Stephen M.},
	month = apr,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.04.07.030213}
}
@inproceedings{kreshuk_who_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Who {Is} {Talking} to {Whom}: {Synaptic} {Partner} {Detection} in {Anisotropic} {Volumes} of {Insect} {Brain}},
	isbn = {978-3-319-24553-9},
	shorttitle = {Who {Is} {Talking} to {Whom}},
	doi = {10.1007/978-3-319-24553-9_81},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} -- {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Kreshuk, Anna and Funke, Jan and Cardona, Albert and Hamprecht, Fred A.},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro},
	year = {2015},
	keywords = {Circuit reconstruction, electron microscopy, graphical model},
	pages = {661--668}
}
@article{buhmann_synaptic_2018,
	title = {Synaptic partner prediction from point annotations in insect brains},
	url = {http://arxiv.org/abs/1806.08205},
	urldate = {2020-09-17},
	journal = {arXiv:1806.08205 [cs]},
	author = {Buhmann, Julia and Krause, Renate and Lentini, Rodrigo Ceballos and Eckstein, Nils and Cook, Matthew and Turaga, Srinivas and Funke, Jan},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.08205},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{buhmann_automatic_2020,
	title = {Automatic {Detection} of {Synaptic} {Partners} in a {Whole}-{Brain} {Drosophila} {EM} {Dataset}},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2019.12.12.874172v2},
	doi = {10.1101/2019.12.12.874172},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Buhmann, Julia and Sheridan, Arlo and Gerhard, Stephan and Krause, Renate and Nguyen, Tri and Heinrich, Larissa and Schlegel, Philipp and Lee, Wei-Chung Allen and Wilson, Rachel and Saalfeld, Stephan and Jefferis, Gregory and Bock, Davi and Turaga, Srinivas and Cook, Matthew and Funke, Jan},
	month = mar,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2019.12.12.874172}
}
@article{huang_fully-automatic_2018,
	title = {Fully-{Automatic} {Synapse} {Prediction} and {Validation} on a {Large} {Data} {Set}},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00087/full},
	doi = {10.3389/fncir.2018.00087},
	language = {English},
	urldate = {2020-09-17},
	journal = {Frontiers in Neural Circuits},
	author = {Huang, Gary B. and Scheffer, Louis K. and Plaza, Stephen M.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {connectomics, deep learning, Drosophila, Evaluation, synapse prediction}
}
@article{plaza_analyzing_2018,
	title = {Analyzing {Image} {Segmentation} for {Connectomics}},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243088/},
	doi = {10.3389/fncir.2018.00102},
	urldate = {2020-09-23},
	journal = {Frontiers in Neural Circuits},
	author = {Plaza, Stephen M. and Funke, Jan},
	month = nov,
	year = {2018},
	pmid = {30483069},
	pmcid = {PMC6243088}
}
@article{kornfeld_em_2017,
	title = {{EM} connectomics reveals axonal target variation in a sequence-generating network},
	volume = {6},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.24364},
	doi = {10.7554/eLife.24364},
	urldate = {2020-10-05},
	journal = {eLife},
	author = {Kornfeld, Jörgen and Benezra, Sam E and Narayanan, Rajeevan T and Svara, Fabian and Egger, Robert and Oberlaender, Marcel and Denk, Winfried and Long, Michael A},
	editor = {Svoboda, Karel},
	month = mar,
	year = {2017},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {birdsong, connectomics, neural sequences, synfire chains, zebra finch},
	pages = {e24364}
}
@article{schneider-mizell_chandelier_2020,
	title = {Chandelier cell anatomy and function reveal a variably distributed but common signal},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.31.018952v1},
	doi = {10.1101/2020.03.31.018952},
	language = {en},
	urldate = {2020-10-05},
	journal = {bioRxiv},
	author = {Schneider-Mizell, Casey M. and Bodor, Agnes L. and Collman, Forrest and Brittain, Derrick and Bleckert, Adam A. and Dorkenwald, Sven and Turner, Nicholas L. and Macrina, Thomas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Zhuang, Jun and Nandi, Anirban and Hu, Brian and Buchanan, JoAnn and Takeno, Marc M. and Torres, Russel and Mahalingam, Gayathri and Bumbarger, Daniel J. and Li, Yang and Chartrand, Tom and Kemnitz, Nico and Silversmith, William M. and Ih, Dodam and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Popovych, Sergiy and Wong, William and Castro, Manuel and Jordan, Chris S. and Froudarakis, Emmanouil and Becker, Lynne and Suckow, Shelby and Reimer, Jacob and Tolias, Andreas S. and Anastassiou, Costas and Seung, H. Sebastian and Reid, R. Clay and Costa, Nuno Maçarico da},
	month = apr,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.03.31.018952}
}
@article{yin_petascale_2020,
	title = {A petascale automated imaging pipeline for mapping neuronal circuits with high-throughput transmission electron microscopy},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-18659-3},
	doi = {10.1038/s41467-020-18659-3},
	language = {en},
	number = {1},
	urldate = {2020-10-05},
	journal = {Nature Communications},
	author = {Yin, Wenjing and Brittain, Derrick and Borseth, Jay and Scott, Marie E. and Williams, Derric and Perkins, Jedediah and Own, Christopher S. and Murfitt, Matthew and Torres, Russel M. and Kapner, Daniel and Mahalingam, Gayathri and Bleckert, Adam and Castelli, Daniel and Reid, David and Lee, Wei-Chung Allen and Graham, Brett J. and Takeno, Marc and Bumbarger, Daniel J. and Farrell, Colin and Reid, R. Clay and da Costa, Nuno Macarico},
	month = oct,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {4949}
}
@article{rand_objective_1971,
	title = {Objective {Criteria} for the {Evaluation} of {Clustering} {Methods}},
	volume = {66},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2284239},
	doi = {10.2307/2284239},
	abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
	number = {336},
	urldate = {2020-10-05},
	journal = {Journal of the American Statistical Association},
	author = {Rand, William M.},
	year = {1971},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {846--850}
}
@article{turner-evans_insect_2016,
	title = {The insect central complex},
	volume = {26},
	issn = {09609822},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0960982216303232},
	doi = {10.1016/j.cub.2016.04.006},
	language = {en},
	number = {11},
	urldate = {2020-10-08},
	journal = {Current Biology},
	author = {Turner-Evans, Daniel B. and Jayaraman, Vivek},
	month = jun,
	year = {2016},
	pages = {R453--R457},
	file = {Turner-Evans and Jayaraman - 2016 - The insect central complex.pdf:/Users/ArloS/Zotero/storage/LR7BU6C3/Turner-Evans and Jayaraman - 2016 - The insect central complex.pdf:application/pdf}
}
@incollection{maitin-shepard_combinatorial_2016,
	title = {Combinatorial {Energy} {Learning} for {Image} {Segmentation}},
	url = {http://papers.nips.cc/paper/6595-combinatorial-energy-learning-for-image-segmentation.pdf},
	urldate = {2020-10-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Maitin-Shepard, Jeremy B and Jain, Viren and Januszewski, Michal and Li, Peter and Abbeel, Pieter},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1966--1974},
	file = {NIPS Full Text PDF:/Users/ArloS/Zotero/storage/VUAW9P2C/Maitin-Shepard et al. - 2016 - Combinatorial Energy Learning for Image Segmentati.pdf:application/pdf;NIPS Snapshot:/Users/ArloS/Zotero/storage/A5AP8YLR/6595-combinatorial-energy-learning-for-image-segmentation.html:text/html}
}
@article{nguyen2020daisy,
	title={Daisy: A Library for Block-Wise Task Scheduling for Large nD Volumes},
	author={Nguyen, Tri and Malin-Mayor, Caroline and Patton, William and Funke, Jan},
	journal={in preparation},
	year={2020}
}
@article{funke_ted_2017,
	series = {Image {Processing} for {Biologists}},
	title = {{TED}: {A} {Tolerant} {Edit} {Distance} for segmentation evaluation},
	volume = {115},
	issn = {1046-2023},
	shorttitle = {{TED}},
	url = {http://www.sciencedirect.com/science/article/pii/S1046202316305072},
	doi = {10.1016/j.ymeth.2016.12.013},
	language = {en},
	urldate = {2020-10-28},
	journal = {Methods},
	author = {Funke, Jan and Klein, Jonas and Moreno-Noguer, Francesc and Cardona, Albert and Cook, Matthew},
	month = feb,
	year = {2017},
	keywords = {Computer vision, Electron microscopy, Evaluation, Learning, Neuron segmentation, Segmentation},
	pages = {119--127},
	file = {ScienceDirect Full Text PDF:/Users/ArloS/Zotero/storage/P4UBBSN6/Funke et al. - 2017 - TED A Tolerant Edit Distance for segmentation eva.pdf:application/pdf;ScienceDirect Snapshot:/Users/ArloS/Zotero/storage/YSBFR2GJ/S1046202316305072.html:text/html}
}
@inproceedings{plaza_focused_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Focused {Proofreading} to {Reconstruct} {Neural} {Connectomes} from {EM} {Images} at {Scale}},
	isbn = {978-3-319-46976-8},
	doi = {10.1007/978-3-319-46976-8_26},
	language = {en},
	booktitle = {Deep {Learning} and {Data} {Labeling} for {Medical} {Applications}},
	publisher = {Springer International Publishing},
	author = {Plaza, Stephen M.},
	editor = {Carneiro, Gustavo and Mateus, Diana and Peter, Loïc and Bradley, Andrew and Tavares, João Manuel R. S. and Belagiannis, Vasileios and Papa, João Paulo and Nascimento, Jacinto C. and Loog, Marco and Lu, Zhi and Cardoso, Jaime S. and Cornebise, Julien},
	year = {2016},
	keywords = {Automatic Segmentation, Edge Probability, Initial Segmentation, Optic Lobe, Synaptic Connection},
	pages = {249--258},
	file = {Springer Full Text PDF:/Users/ArloS/Zotero/storage/JNG5MEDQ/Plaza - 2016 - Focused Proofreading to Reconstruct Neural Connect.pdf:application/pdf}
}
@article{gallusser_2020,
  title={Deep-Learning-Based Automatic Organelle Segmentation in 3D Electron Microscopy Datasets},
  author={Gallusser, Benjamin and Vadakkan, Tegy John and Sahasrabudhe, Mihir and Di Caprio, Giuseppe and Kirchhausen, Tom},
  journal={in preparation},
  year={2020}
}
@article{hildebrand_whole-brain_2017,
	title = {Whole-brain serial-section electron microscopy in larval zebrafish},
	volume = {545},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature22356},
	doi = {10.1038/nature22356},
	abstract = {A complete larval zebrafish brain is examined and its myelinated axons reconstructed using serial-section electron microscopy, revealing remarkable symmetry and providing a valuable resource.},
	language = {en},
	number = {7654},
	urldate = {2020-11-11},
	journal = {Nature},
	author = {Hildebrand, David Grant Colburn and Cicconet, Marcelo and Torres, Russel Miguel and Choi, Woohyuk and Quan, Tran Minh and Moon, Jungmin and Wetzel, Arthur Willis and Scott Champion, Andrew and Graham, Brett Jesse and Randlett, Owen and Plummer, George Scott and Portugues, Ruben and Bianco, Isaac Henry and Saalfeld, Stephan and Baden, Alexander David and Lillaney, Kunal and Burns, Randal and Vogelstein, Joshua Tzvi and Schier, Alexander Franz and Lee, Wei-Chung Allen and Jeong, Won-Ki and Lichtman, Jeff William and Engert, Florian},
	month = may,
	year = {2017},
	note = {Number: 7654
Publisher: Nature Publishing Group},
	pages = {345--349},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/MXUPZ29N/Hildebrand et al. - 2017 - Whole-brain serial-section electron microscopy in .pdf:application/pdf;Snapshot:/Users/ArloS/Zotero/storage/NYCZKHWR/nature22356.html:text/html}
}
@article{saalfeld2009catmaid,
	title={CATMAID: collaborative annotation toolkit for massive amounts of image data},
	author={Saalfeld, Stephan and Cardona, Albert and Hartenstein, Volker and Toman{\v{c}}{\'a}k, Pavel},
	journal={Bioinformatics},
	volume={25},
	number={15},
	pages={1984--1986},
	year={2009},
	publisher={Oxford University Press}
}
@article{boergens_webknossos_2017,
	title = {{webKnossos}: efficient online {3D} data annotation for connectomics},
	volume = {14},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{webKnossos}},
	url = {https://www.nature.com/articles/nmeth.4331},
	doi = {10.1038/nmeth.4331},
	language = {en},
	number = {7},
	urldate = {2020-11-16},
	journal = {Nature Methods},
	author = {Boergens, Kevin M. and Berning, Manuel and Bocklisch, Tom and Bräunlein, Dominic and Drawitsch, Florian and Frohnhofen, Johannes and Herold, Tom and Otto, Philipp and Rzepka, Norman and Werkmeister, Thomas and Werner, Daniel and Wiese, Georg and Wissler, Heiko and Helmstaedter, Moritz},
	month = jul,
	year = {2017},
	note = {Number: 7
Publisher: Nature Publishing Group},
	pages = {691--694},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/VDHQQKL2/Boergens et al. - 2017 - webKnossos efficient online 3D data annotation fo.pdf:application/pdf;Snapshot:/Users/ArloS/Zotero/storage/CQJI3QWJ/nmeth.html:text/html}
}
@article{berger_vast_2018,
	title = {{VAST} ({Volume} {Annotation} and {Segmentation} {Tool}): {Efficient} {Manual} and {Semi}-{Automatic} {Labeling} of {Large} {3D} {Image} {Stacks}},
	volume = {12},
	issn = {1662-5110},
	shorttitle = {{VAST} ({Volume} {Annotation} and {Segmentation} {Tool})},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00088/full},
	doi = {10.3389/fncir.2018.00088},
	language = {English},
	urldate = {2020-11-16},
	journal = {Frontiers in Neural Circuits},
	author = {Berger, Daniel R. and Seung, H. Sebastian and Lichtman, Jeff W.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {CLEM, connectomics, proofreading, segmentation, serial section electron microscopy, tracing, TrakEM2, visualization, Voxel},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/DBQWS7DN/Berger et al. - 2018 - VAST (Volume Annotation and Segmentation Tool) Ef.pdf:application/pdf}
}
@article{zhao_neutu_2018,
	title = {{NeuTu}: {Software} for {Collaborative}, {Large}-{Scale}, {Segmentation}-{Based} {Connectome} {Reconstruction}},
	volume = {12},
	issn = {1662-5110},
	shorttitle = {{NeuTu}},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00101/full},
	doi = {10.3389/fncir.2018.00101},
	language = {English},
	urldate = {2020-11-16},
	journal = {Frontiers in Neural Circuits},
	author = {Zhao, Ting and Olbris, Donald J. and Yu, Yang and Plaza, Stephen M.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {connectome, Electron microscopy, NeuTu, proofreading, Segmentation (Image processing)},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/4WPFZEFA/Zhao et al. - 2018 - NeuTu Software for Collaborative, Large-Scale, Se.pdf:application/pdf}
}


</script>
<!--


-->
<script language="javascript" type="text/javascript" src="lib/p5.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/p5.dom.js"></script>
<script language="javascript" type="text/javascript" src="lib/numjs.js"></script>
<script src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/utils.js"></script>
