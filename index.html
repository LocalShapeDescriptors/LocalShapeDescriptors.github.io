<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  <!-- roboto font -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:300' rel='stylesheet' type='text/css'>

  <link rel="icon" href="assets/img/aclsd.png" type="image/png">

  <meta name="theme-color" content="#ffffff" />

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-141682504-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-141682504-1');
  </script>


  <!-- SEO -->
  <meta property="og:title" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="Auxiliary learning for large scale connectomics" />
  <meta property="og:image" content="assets/img/lsds_header.jpeg" />
  <meta property="og:url" content="https://localshapedescriptors.github.io" />

  <!-- Twitter Card data -->
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta name="twitter:description" content="" />
  <meta property="og:site_name" content="Local Shape Descriptors for Neuron Segmentation" />
  <meta name="twitter:image" content="assets/img/lsds_header.jpeg" />

</head>

<style>

body {
  margin: 0px;
}

.row {
  display: flex;
  flex-wrap: wrap;
  padding: 0 4px;
}

/* Create four equal columns that sits next to each other */
.column {
  flex: 25%;
  max-width: 25%;
  padding: 0 4px;
}

.column img {
  vertical-align: middle;
  width: 100%;
}

/* Responsive layout - makes a two column-layout instead of four columns */
@media screen and (max-width: 800px) {
  .column {
    flex: 50%;
    max-width: 50%;
  }
}

/* Responsive layout - makes the two columns stack on top of each other instead of next to each other */
@media screen and (max-width: 600px) {
  .column {
    flex: 100%;
    max-width: 100%;
  }
}

.image-grid {
  display: flex;
  flex-wrap: wrap;
  width: 90%;
  margin: 0 auto;
}
.grid-image {
  display: block;
  flex-basis: 100%;
  width: 100%;
  height: 100%;
  padding: 10px;
  box-sizing: border-box;
}

@media only screen and (min-width: 640px) {
  .grid-image {
    flex-basis: 50%;
  }
}

@media only screen and (min-width: 960px) {
  .grid-image {
    flex-basis: 33.333%;
  }
}

@media only screen and (min-width: 1280px) {
  .grid-image {
    flex-basis: 25%;
  }
}

@media only screen and (min-width: 1600px) {
  .grid-image {
    flex-basis: 20%;
  }
}

.a {
  position: absolute;
  z-index: 3;
}

.b {
  position: absolute;
  z-index: 2;
}

.img-magnifier-glass {
  position: absolute;
  border: 2px solid #000;
  border-radius: 50%;
  cursor: none;
  /*Set the size of the magnifier glass:*/
  width: 50px;
  height: 50px;
  z-index: 1;
}

.accordion-container {
  position: relative;
  width: 100%;
  overflow: hidden;
  margin: auto;
}

.responsive-container {
  position: relative;
  width: 100%;
  overflow: hidden;
  margin: auto;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
}

.test-container {
  position: relative;
  width: 100%;
  height: 100%;
  overflow: hidden;
  margin: auto;
  border: 5px solid red;
  padding-top: 56.25%; /* 16:9 Aspect Ratio */
}

.plot-container {
  position: relative;
  width: 100%;
  height: 650px;
  overflow: hidden;
  margin: auto;
}

.responsive-iframe {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  right: 0;
  width: 100%;
  height: 100%;
  display: block;
  border: none;
}

.scroll-down {
  width: 80px;
  height: 40px;
  right: 10px;
  bottom: 10px;
  position: absolute;
  font-family: "Roboto","Helvetica Neue",Helvetica,Arial,sans-serif;
  font-size: 12px;
  font-weight: 300;
  color: #FFFFFF;
  opacity: 0;
  -webkit-transition: opacity 2s ease-in;
  -moz-transition: opacity 2s ease-in;
  -o-transition: opacity 2s ease-in;
  -ms-transition: opacity 2s ease-in;
  transition: opacity 2s ease-in;
}

.scroll-down span {
  margin-top: 5px;
  position: absolute;
  left: 50%;
  transform: translate(-100%, 0) rotate(45deg);
  transform-origin: 100% 100%;
  height: 2px;
  width: 10px;
  background: #FFFFFF;
}

.scroll-down span:nth-of-type(2) {
  transform-origin: 0 100%;
  transform: translate(0, 0) rotate(-45deg);
}

.spinner {
  position: absolute;
  height: 160px;
  width: 160px;
  -webkit-animation: rotation .6s infinite linear;
  -moz-animation: rotation .6s infinite linear;
  -o-animation: rotation .6s infinite linear;
  animation: rotation .6s infinite linear;
  border-left: 6px solid rgba(0, 174, 239, .15);
  border-right: 6px solid rgba(0, 174, 239, .15);
  border-bottom: 6px solid rgba(0, 174, 239, .15);
  border-top: 6px solid rgba(0, 174, 239, .8);
  border-radius: 100%;
  top: calc(50% - 100px);
  left: calc(50% - 80px);
  right: auto;
  bottom: auto;
}

.accordion {
  margin: auto;
  position: relative;
}
.accordion input {
  display: none;
}

.tooltip {
    position: relative;
    display: inline-block;
    border-bottom: 1px dotted black;
}

.tooltip .tooltiptext {
    visibility: hidden;
    width: 170px;
    background-color: black;
    color: #fff;
    text-align: center;
    border-radius: 6px;
    padding: 5px 0;

    /* Position the tooltip */
    position: absolute;
    z-index: 1;
}

.tooltip:hover .tooltiptext {
    visibility: visible;
}

.box {
  position: relative;
  background: white;
  height: 64px;
  transition: all .15s ease-in-out;
}

.box::before {
  content: '';
  position: absolute;
  display: block;
  top: 0;
  bottom: 0;
  left: 0;
  right: 0;
  pointer-events: none;
  box-shadow: 0 -1px 0 #e5e5e5,0 0 2px rgba(0,0,0,.12),0 2px 4px rgba(0,0,0,.24);
}

header.box {
  background: #00BCD4;
  z-index: 100;
  cursor: initial;
  box-shadow: 0 -1px 0 #e5e5e5,0 0 2px -2px rgba(0,0,0,.12),0 2px 4px -4px rgba(0,0,0,.24);
}

header.box-title {
  margin: 0;
  font-weight: normal;
  font-size: 16pt;
  color: white;
  cursor: initial;
}

.box-title {
  width: calc(100% - 40px);
  height: 64px;
  line-height: 64px;
  padding: 0 20px;
  display: inline-block;
  cursor: pointer;
  -webkit-touch-callout: none;-webkit-user-select: none;-khtml-user-select: none;-moz-user-select: none;-ms-user-select: none;user-select: none;
}

.box-content {
  width: 100%;
  height: 100%;
  display: none;
  overflow: hidden;
  margin: auto;
}

.box-close {
  position: absolute;
  height: 64px;
  width: 100%;
  top: 0;
  left: 0;
  cursor: pointer;
  display: none;
}
input:checked + .box {
  height: auto;
  margin: 16px 0;
  box-shadow: 0 0 6px rgba(0,0,0,.16),0 6px 12px rgba(0,0,0,.32);
}
input:checked + .box .box-title {
  border-bottom: 1px solid rgba(0,0,0,.18);
}
input:checked + .box .box-content,
input:checked + .box .box-close {
  display: inline-block;
}

.arrows section .box-title {
  padding-left: 44px;
  width: calc(100% - 64px);
}
.arrows section .box-title:before {
  position: absolute;
  display: block;
  content: '\203a';
  font-size: 18pt;
  left: 20px;
  top: -2px;
  transition: transform .15s ease-in-out;
  color: rgba(0,0,0,.54);
}
input:checked + section.box .box-title:before {
  transform: rotate(90deg);
}

@-webkit-keyframes rotation {
  from {
    -webkit-transform: rotate(0deg);
  }
  to {
    -webkit-transform: rotate(359deg);
  }
}
.transparent {
  opacity: 0;
}

figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: left;
}

dt-article figcaption {
  padding: 0.5em;
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
  text-align: center;
}

dt-article figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

dt-article figcaption b {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

*.unselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
}
*.svgunselectable {
    -moz-user-select: -moz-none;
    -khtml-user-select: none;
    -webkit-user-select: none;
    -o-user-select: none;
    user-select: none;
    background: none;
    pointer-events: none;
}

.btn-group button {
  background-color: orange;
  border: 1px solid #FF6C00;
  color: white; /* White text */
  padding: 5px 12px; /* Some padding */
  cursor: pointer; /* Pointer/hand icon */
  float: center; /* Float the buttons side by side */
}

#toc_container {
  float: left;
  width: auto;
  background: #eee;
  font-size: 0.8em;
  padding: 1em 1em;
  margin: 0 1em 0 1em;
  border: 1px solid #aaa;
}

.toc_title {
    font-weight: 700;
    text-align: center;
}

#toc_container ul>li {
margin: 0 0 .6em 0;
list-style-type: none;
}

#toc_container a {
text-decoration: none;
color: #3D5AFE;
}

/* Add a background color on hover */
.btn-group button:hover {
  background-color: #FF6C00;
}

#toc_scroll {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  border: 2px solid #008CBA;
  outline: none;
  background-color: white;
  color: black;
  cursor: pointer;
  padding: 5px;
  font-size: 18px;
  border-radius: 8px;
}

#toc_scroll:hover {
  background-color: #aaa;
}

</style>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">

<script src="lib/mobile-detect.min.js"></script>
<script src="lib/template.v1.js"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

<script type="text/front-matter">
  title: "LSDs"
  description: ""
</script>
<body>

<div style="text-align: center;">
  <img class="b-lazy"
       src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
       data-src="assets/img/small/lsds_header.jpeg"
       srcset="assets/img/small/lsds_header.jpeg 320w,
       assets/img/medium/lsds_header.jpeg 800w,
       assets/img/large/lsds_header.jpeg 1200w,
       assets/img/xlarge/lsds_header.jpeg 1920w"
       sizes="100vw"
       alt="LSD direction vectors">
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="50%"><figcaption style="text-align: center;">Direction vectors of
      the LSDs on a single neuron from the fly visual system. Colors correspond
      to the direction in which the neuronal processes travel.</figcaption></td>
  </tr></table>
</div>

<dt-article class="centered" id="dtbody">

<dt-byline class="l-page transparent"></dt-byline>
<h1>Local Shape Descriptors for Neuron Segmentation</h1>
<p></p>
<dt-byline class="l-page" id="authors_section" hidden>
<div class="byline">
  <div class="authors">
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=Md__FUQAAAAJ&hl=en&oi=ao">Arlo Sheridan</a>
        <a class="affiliation" href="https://www.salk.edu/science/core-facilities/advanced-biophotonics">Salk Institute</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=AnU408sAAAAJ&hl=en">Tri M. Nguyen</a>
        <a class="affiliation" href="https://www.lee.hms.harvard.edu/">Harvard University</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=gBcB_UUAAAAJ&hl=en">Diptodip Deb</a>
        <a class="affiliation" href="https://www.janelia.org/lab/turaga-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=y2s07ssAAAAJ&hl=en">Wei-Chung Allen Lee</a>
        <a class="affiliation" href="https://www.lee.hms.harvard.edu/">Harvard University</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=oSGyzt4AAAAJ&hl=en">Stephan Saalfeld</a>
        <a class="affiliation" href="https://www.janelia.org/lab/saalfeld-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=V_NdI3sAAAAJ&hl=en">Srini Turaga</a>
        <a class="affiliation" href="https://www.janelia.org/lab/turaga-lab">HHMI Janelia</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=_ZLy6tEAAAAJ&hl=en">Uri Manor</a>
        <a class="affiliation" href="https://www.salk.edu/science/core-facilities/advanced-biophotonics">Salk Institute</a>
    </div>
    <div class="author">
        <a class="name" href="https://scholar.google.com/citations?user=7rqAapgAAAAJ&hl=en">Jan Funke</a>
        <a class="affiliation" href="https://www.janelia.org/lab/funke-lab">HHMI Janelia</a>
    </div>
  </div>
  <div class="date">
    <div class="month">LSD</div>
    <div class="year" style="color: #2AAFCF;"><a href="https://github.com/funkelab/lsd_paper" target="_blank">Paper</a></div>
  </div>
</div>
</dt-byline>
</dt-byline>

<button onclick="topFunction()" id="toc_scroll" title="Scroll to toc">Contents</button>

<script>

jQuery(document).ready(function($){
    $('[data-toggle="popover"]').popover();
});

var md = new MobileDetect(window.navigator.userAgent);

//Get the button
var mybutton = document.getElementById("toc_scroll");

// When the user scrolls down 2000px from the top of the document, show the button
window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  var toc = document.getElementById("toc_container");
  if (window.scrollY > (toc.offsetTop + toc.offsetHeight)) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}

// When the user clicks on the button, scroll to the top of the document
function topFunction() {
  document.getElementById("toc_container").scrollIntoView();
}

function magnify(imgID, zoom, secondim) {
  var img, glass, w, h, bw, simg;
  img = document.getElementById(imgID);
  simg = document.getElementById(secondim);
  /*create magnifier glass:*/
  glass = document.createElement("DIV");
  glass.setAttribute("class", "img-magnifier-glass");
  /*insert magnifier glass:*/
  img.parentElement.insertBefore(glass, img);
  /*set background properties for the magnifier glass:*/
  glass.style.backgroundImage = "url('" + simg.src + "')";
  glass.style.backgroundRepeat = "no-repeat";
  glass.style.backgroundSize = (img.width * zoom) + "px " + (img.height * zoom) + "px";
  bw = 3;
  w = glass.offsetWidth / 2;
  h = glass.offsetHeight / 2;
  /*execute a function when someone moves the magnifier glass over the image:*/
  glass.addEventListener("mousemove", moveMagnifier);
  img.addEventListener("mousemove", moveMagnifier);
  /*and also for touch screens:*/
  glass.addEventListener("touchmove", moveMagnifier);
  img.addEventListener("touchmove", moveMagnifier);
  function moveMagnifier(e) {
    var pos, x, y;
    /*prevent any other actions that may occur when moving over the image*/
    e.preventDefault();
    /*get the cursor's x and y positions:*/
    pos = getCursorPos(e);
    x = pos.x;
    y = pos.y;
    /*prevent the magnifier glass from being positioned outside the image:*/
    if (x > img.width - (w / zoom)) {x = img.width - (w / zoom);}
    if (x < w / zoom) {x = w / zoom;}
    if (y > img.height - (h / zoom)) {y = img.height - (h / zoom);}
    if (y < h / zoom) {y = h / zoom;}
    /*set the position of the magnifier glass:*/
    glass.style.left = (x - w) + "px";
    glass.style.top = (y - h) + "px";
    /*display what the magnifier glass "sees":*/
    glass.style.backgroundPosition = "-" + ((x * zoom) - w + bw) + "px -" + ((y * zoom) - h + bw) + "px";
  }
  function getCursorPos(e) {
    var a, x = 0, y = 0;
    e = e || window.event;
    /*get the x and y positions of the image:*/
    a = img.getBoundingClientRect();
    /*calculate the cursor's x and y coordinates, relative to the image:*/
    x = e.pageX - a.left;
    y = e.pageY - a.top;
    /*consider any page scrolling:*/
    x = x - window.pageXOffset;
    y = y - window.pageYOffset;
    return {x : x, y : y};
  }
}

</script>
<h2>Abstract</h2>
<p>We present a simple, yet effective, auxiliary learning task for the problem of
neuron segmentation in electron microscopy volumes. The auxiliary task
consists of the prediction of Local Shape Descriptors (LSDs), which we combine
with conventional voxel-wise direct neighbor affinities for neuron boundary
detection. The shape descriptors are designed to capture local statistics
about the neuron to be segmented, such as diameter, elongation, and direction.
On a large study comparing several existing methods across various specimen,
imaging techniques, and resolutions, we find that auxiliary learning of LSDs
consistently increases segmentation accuracy of affinity-based methods over a
range of metrics. Furthermore, the addition of LSDs promotes affinity-based
segmentation methods to be on par with the current state of the art for neuron
segmentation (Flood-Filling Networks, FFN), while being two orders of
magnitudes more efficient - a critical requirement for the processing of
future petabyte-sized datasets. Implementations of the new auxiliary learning
task, network architectures, training, prediction, and evaluation code, as
well as the datasets used in this study are publicly available as a benchmark
for future method contributions.</p>
<hr>
<div id="toc_container">
<p class="toc_title">Contents</p>
<ul class="toc_list">
  <li><a href="#background">1. Background</a>
  <ul>
    <li><a href="#connectomics">1.1 Connectomics</a></li>
    <li><a href="#neuron_segmentation">1.2 Neuron Segmentation</a></li>
    <li><a href="#related_work">1.3 Related Work</a></li>
    <li><a href="#contributions">1.4 Contributions</a></li>
  </ul>
</li>
<li><a href="#methods">2. Methods</a></li>
  <ul>
    <li><a href="#local_shape_descriptors">2.1 Local Shape Descriptors</a></li>
    <li><a href="#network_architectures">2.2 Network Architectures</a></li>
  </ul>
<li><a href="#results">3. Results</a></li>
  <ul>
    <li><a href="#metrics">3.1 Metrics</a></li>
    <li><a href="#datasets">3.2 Datasets</a></li>
    <li><a href="#accuracy">3.3 Accuracy</a></li>
    <li><a href="#throughput">3.4 Throughput</a></li>
  </ul>
<li><a href="#discussion">4. Discussion</a></li>
  <ul>
    <li><a href="#metric_eval">4.1 Metric Evaluation</a></li>
    <li><a href="#auxiliary_learning">4.2 Auxiliary Learning</a></li>
    <li><a href="#auto_context">4.3 Auto-Context</a></li>
    <li><a href="#masking">4.4 Masking</a></li>
  </ul>
<li><a href="#conclusions">5. Conclusions</a></li>
<li><a href="#tldr">6. Tl;dr</a></li>
<li><a href="#acknowledgements">7. Acknowledgements</a></li>
<li><a href="#code">8. Code</a></li>
<li><a href="#supplementary">9. Supplementary</a></li>
<li><a href="#references">10. References</a></li>
</ul>
</div>
<h2 id="background">Background</h2>
<h3 id="connectomics">Connectomics</h3>
<p>Connectomics is an emerging field which integrates multiple domains including
neuroscience, microscopy, and computer science. The overarching goal is to
provide insights about the brain at resolutions which are not achievable with
other approaches. The ability to study neural structures at this scale will
hopefully lead to a better understanding of brain disorders, and subsequently
advance medical approaches towards finding treatments &amp; cures.</p>
<p>The basic idea is to produce &quot;connectomes&quot; which are essentially maps of the
brain. These maps, or &quot;wiring diagrams&quot;, give scientists the ability to see how
every neuron interacts through synaptic connections. They can be used to
complement existing techniques <dt-cite
key="schlegel_synaptic_2016,turner-evans_neuroanatomical_2020"></dt-cite> and
drive future experiments <dt-cite
key="schneider-mizell_quantitative_2016,motta_dense_2019,bates_complete_2020"></dt-cite>.</p>
<p>Okay, but how are the brain maps generated?</p>
<p>Before generating neural wiring diagrams, we first need to acquire the brain
tissue to use. Currently, only Electron Microscopy (EM) allows imaging of neural
tissue at a resolution sufficient to see individual synapses. After extracting a
brain (for example, from a fruit fly), the tissue is generally stained with
heavy metals to increase contrast between structures of interest (i.e neuron
membranes). Once stained, the tissue is imaged with an electron microscope.
There are a few types of EM imaging approaches. Three popular techniques are
serial section transmission EM (ssTEM), serial block-face scanning EM (SBF-SEM)
and focused ion beam scanning EM (FIB-SEM). The former two methods involve
slicing the brain into super thin (e.g 20 nanometer) sections. The latter uses
an ion beam to erode the tissue. In either case, electrons are shot at the
tissue to produce an image of the data. This is a way oversimplified
explanation, for a better overview of these EM imaging techniques (and others),
see <dt-cite key="briggman_volume_2012"> this paper</dt-cite>, specifically
Figure 1.</p>
<p>Sweet! Let's image a human brain and be done with it.</p>
<p>Unfortunately, by imaging brains at such high resolution, the resulting data is
massive. Let's consider the fruit fly example.  A full adult fruit fly brain
(<strong>FAFB</strong>) imaged with ssTEM <dt-cite key="zheng_complete_2018"></dt-cite> at a
pixel resolution of ~4 nanometers and ~40 nanometer thick sections, comprises
~213 teravoxels of data (~50 teravoxels of actual brain tissue)<dt-cite
key="heinrich_synaptic_2018"></dt-cite>. For reference, a voxel is a volumetric
pixel, and the &quot;tera&quot; prefix means 10<sup>12</sup>. So, one fly brain contains
upwards of 213,000,000,000,000 volumetric pixels. To put that in perspective,
<dt-cite key="abbott_mind_2020">Abbott et al.</dt-cite> argue that, assuming a
scale where 1000 cubic microns is equivalent to 1 centimeter, a fruit fly brain
would comprise the length of 6 and a half Boeing 747 aeroplanes. This still
pales in comparison to a mouse brain which would be the distance from Boston to
Lisbon, and require the acquisition of 1 million terabytes of data.</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/scale.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="50%"><figcaption style="text-align: center;">Scale perspective. A
fruit fly brain imaged at synaptic resolution takes up 100's of terabytes of
storage space. It allows us to see fine structures such as neural plasma
membranes (pink arrow), synapses (blue arrow), vesicles (green arrow) and
mitochondria (orange arrow). 3D fruit fly model kindly provided by <a
href="https://scholar.google.com/citations?user=ir1vhA8AAAAJ&hl=en"
target="_blank">Igor Siwanowicz</a></figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<p>Try navigating the fly brain in an interactive <a
href="https://github.com/google/neuroglancer" target="_blank">Neuroglancer</a>
viewer (click question mark for controls). Think, Google Earth for brains:</p>
<html>
  <body>
    <div class="accordion-container">
      <nav class="accordion arrows">
        <input type="radio" name="accordion" id="cb1" checked/>
        <section class="box">
          <label class="box-title" for="cb1">Full adult fly brain interactive viewer</label>
          <label class="box-close" for="acc-open"></label>
          <input type="radio" name="accordion" id="acc-open"/>
          <div class="box-content">
            <div class="responsive-container">
              <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B4e-8%2C%22m%22%5D%7D%2C%22position%22:%5B63696.5703125%2C30152.005859375%2C3242.8369140625%5D%2C%22crossSectionScale%22:413.08059520030787%2C%22projectionOrientation%22:%5B-0.06840167939662933%2C-0.4631274342536926%2C-0.204045370221138%2C0.8597671985626221%5D%2C%22projectionScale%22:124585.76409043159%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-fafb-data/fafb_v14/fafb_v14_clahe%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#0088ff%22%2C%22name%22:%22fafb_v14_clahe%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22fafb_v14_clahe%22%7D%2C%22layout%22:%224panel%22%2C%22partialViewport%22:%5B0%2C0%2C1%2C1%5D%7D" "></iframe>
            </div>
          </div>
        </section>
      </nav>
    </div>
  </body>
</html>
<p>Okay, now we have the data, so how do we create the wiring diagrams?</p>
<p>To create a wiring diagram, we need to reconstruct all of the neurons and their
synaptic connections. This process can be done manually - which consists of
human annotators navigating these datasets and labeling every neuron and their
synaptic partners using various software <dt-cite
key="saalfeld2009catmaid,boergens_webknossos_2017,berger_vast_2018,zhao_neutu_2018"></dt-cite>.
However, this can become extremely tedious and expensive (<strong>$$$</strong>) given the
size of the datasets. For example, simply reconstructing 129 neurons from
<strong>FAFB</strong> took a team of tracers ~60 days to complete<dt-cite
key="zheng_complete_2018"></dt-cite>. Given that a fruit fly has ~100,000
neurons, purely manual reconstruction of connectomes is obviously infeasible.</p>
<p>Consequently, methods have been developed to automate this process. From here
on, we will focus on the automatic reconstruction of neurons. To see the current
approaches to synapse detection, check <dt-cite
key="kreshuk_who_2015,heinrich_synaptic_2018,huang_fully-automatic_2018,buhmann_automatic_2020">
these papers</dt-cite> out!</p>
<h3 id="neuron_segmentation">Neuron Segmentation</h3>
<p>Neuron segmentation is the current rate-limiting step for generating large
connectomes. Errors in a neuron segmentation can easily propagate throughout a
dataset as the scale increases, which makes it tedious for humans to proofread
the data without advanced tools.</p>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="seg" />
  <section class="box">
  <label class="box-title" for="seg">Segmentation overview</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div> <p> To better understand the problem of neuron
  segmentation, it is necessary to understand what segmentation is. There are a
  few ways to detect objects in an image. A standard approach is called
  <b>object detection</b> which is a technique used to locate and label objects,
  often resulting in fitting a bounding box to each object. This is a good
  strategy for finding and tracking objects in an image but it neglects the
  volumetric data contained within an object. An alternative method is called
  segmentation in which every pixel of an object is assigned to a class. There
  are two main techniques: <b>semantic segmentation</b> and <b>instance
  segmentation</b>. In semantic segmentation, each pixel is assigned to a
  broader class, for example each car in an image would be assigned to a car
  class and each animal in an image would be assigned to a animal class.
  Conversely, instance segmentation assigns each pixel to a unique label.
  Consider the following example: each shoe in the image can be located (object
  detection), each pixel of each shoe can be assigned to a class indicating the
  type of shoe (semantic segmentation - soccer cleats, boots, sneakers), and
  each pixel can be assigned a unique label indicating the specific shoe: </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img id="shoes" src="assets/img/detection_vs_segmentation_shoes.jpeg" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img id="shoes_vertical" src="assets/img/detection_vs_segmentation_shoes_vertical.jpeg" style="display: block; margin: auto; width: 100%;"/>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<p>Neuron reconstruction is an instance segmentation problem because every pixel of
every neuron needs to be assigned a unique label (in contrast to object
detection and semantic segmentation). For example, given a raw EM image, we
could simply detect mitochondria (object detection), assign all pixels
containing mitochondria to one class (semantic segmentation), or assign all
pixels of each object to a unique class (instance segmentation).</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/detection_vs_segmentation_neurons.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  </div>
  </body>
</html>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons_vertical"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/detection_vs_segmentation_neurons_vertical.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  </div>
  </body>
</html>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="labels" />
  <section class="box">
  <label class="box-title" for="labels">Raw and Labels viewer</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%7D%2C%22position%22:%5B98.5%2C98.5%2C98.5%5D%2C%22crossSectionScale%22:1.0176722261336113%2C%22projectionOrientation%22:%5B-0.21856307983398438%2C-0.4040609300136566%2C-0.18352380394935608%2C0.8690707087516785%5D%2C%22projectionScale%22:484.7402817353436%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/raw%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#fb00ff%22%2C%22name%22:%22raw%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/labels%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C1%2C52%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%7D%7D%2C%22subsources%22:%7B%22default%22:true%2C%22bounds%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#00d9ff%22%2C%22crossSectionRenderScale%22:0.24230351153893748%2C%22colorSeed%22:54670317%2C%22name%22:%22labels%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22raw%22%7D%2C%22layout%22:%224panel%22%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<p>It would be ideal to directly predict unique labels (neurons) in a dataset.
Unfortunately this requires global information which is difficult because
neurons span large distances. Due to the nature of neural networks, field of
views are not large enough to account for downstream changes in a neuron such as
branching and merging. Consequently, alternative approaches aim to solve the
problem locally.</p>
<script>

if (md.is('iPhone')){
  console.log('foo');
  document.getElementById('shoes_vertical').style.display = 'none';
  document.getElementById('neurons_vertical').style.display = 'none';
}
else {
  console.log('moo');
  document.getElementById('shoes').style.display = 'none';
  document.getElementById('neurons').style.display = 'none';
}

</script>
<h3 id="related_work">Related Work</h3>
<p>Most current approaches to neuron segmentation center around producing boundary
maps which are then used to generate unique objects with post-processing steps.
Consider the following example:</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/labels.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">We have four neurons
(A,B,C,D) and we want to assign voxels (squares) to the label they belong to.
Images kindly provided by <a
href="https://scholar.google.com/citations?user=oSGyzt4AAAAJ&hl=en"
target="_blank">Stephan Saalfeld</a>.</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>Foreground / Background
<ul>
<li>It is often sufficient to assign pixels as either foreground or background
and then perform connected components to label unique objects <dt-cite
key="ciresan_deep_2012"></dt-cite>. This technique can fail for neuron
segmentation because the axial resolution of fine-tip neurites is lower.</li>
</ul>
</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/boundaries.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">Naively labeling
foreground voxels (white squares) and background voxels (black squares) would
result in the top part of the yellow neuron (B) being falsely labeled as
background.</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>Nearest-neighbor Affinities
<ul>
<li>One solution is to predict edges between neighboring voxels rather than the
voxels themselves <dt-cite
key="turaga_convolutional_2010,funke_large_2019"></dt-cite>. This ensures
that distal processes receive the same treatment as larger axons. Since
these affinity graphs can be computed locally, they can be parallelized
which leads to performance increases. However, affinities are sensitive to
small errors. A few incorrectly assigned pixels can lead to false merges
during post-processing.</li>
</ul>
</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/affinities.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">The top part of the
yellow neuron would now be correctly assigned. However, slight changes in
boundary predictions could result in subsequent post-processing
errors.</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>Long Range Affinities
<ul>
<li>In order to incorporate more context for the receptive field of the network, a larger affinity neighborhood can be used as an auxiliary learning objective <dt-cite
key="lee_superhuman_2017"></dt-cite>. This theoretically helps to improve
the nearest neighbor affinities and the resulting segmentations.</li>
</ul>
</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/longrange.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">Several increased
neighborhood steps can be used to provide increased context to the nearest
neighbor affinities. This essentially allows the network to see more than it
otherwise would.</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>MALIS loss
<ul>
<li>The above approaches rely on a Euclidean loss. An alternative loss function
(MALIS) <dt-cite key="turaga_maximin_2009"></dt-cite> penalizes topological
errors by minimizing the Rand Index, a measure of similarity between
clusterings <dt-cite key="rand_objective_1971"></dt-cite>. This method has
been further optimized by constraining the loss to a positive and negative
pass followed by growing a maximal spanning tree on the affinity graph
<dt-cite key="funke_large_2019"></dt-cite>.</li>
</ul>
</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/malis.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">Todo: explain this</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>Flood Filling Networks (FFN)
<ul>
<li>The current state of the art approach <dt-cite
key="januszewski_high-precision_2018"></dt-cite> eliminates the need for a
separate post-processing step. After generateing seed points within neurons,
a recurrent CNN iteratively fills each object by predicting which voxels
belong to the same objects as the seeds.</li>
</ul>
</li>
</ul>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/ffn.jpeg"
    style="display: block; margin: auto; width: 50%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
<td width="100%"><figcaption style="text-align: center;">Given seed points
inside a neuron (large white square, for example), the network predicts which
voxels belong to the same neuron (white squares) and which belong to different
neurons (black squares).</figcaption></td>
</tr></table>
  </div>
  </body>
</html>
<ul>
<li>Other approaches
<ul>
<li>
<p>Deep Metric Learning</p>
<ul>
<li>A relatively new method uses deep metric learning to produce dense voxel
embeddings <dt-cite key="lee_learning_2019"></dt-cite>. Voxels within an
objects are similar in embedding space while voxels across objects are
repelled <dt-fn>This method was introduced after experiment completion and
is therefore not compared against.</dt-fn>.</li>
</ul>
</li>
<li>
<p>Watershed / Agglomeration Variants</p>
<ul>
<li>Other approaches <dt-cite
key="ferrari_mutex_2018,beier2017multicut"></dt-cite> aim to improve
segmentations by targeting the downstream graphs generated during
post-processing rather than the boundary labels <dt-fn>We assess boundary
prediction in this paper and do not consider post-processing
strategies.</dt-fn>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<!--<html>-->
  <!--<body>-->
<!--<div style="text-align: center;">-->
<!--<img class="b-lazy"-->
<!--src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==-->
<!--data-src="assets/img/related_methods_vertical.jpeg" style="display: block; margin: auto; width: 100%;"/>-->
<!--<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>-->
<!--</tr></table>-->
  <!--</div>-->
  <!--</body>-->
<!--</html>-->
<h3 id="contributions">Contributions</h3>
<ul>
<li>
<p>Local Shape Descriptors (LSDs)</p>
<ul>
<li>We introduce LSDs, a 10-Dimensional <a
href="#local_shape_descriptors">embedding</a> for each voxel which encodes
local object shape. We train several <a href="#network_architectures">neural
networks</a> to predict LSDs as an auxiliary learning objective along with
direct neighbor affinity predictions <dt-fn>Similar to Long Range
affinities.</dt-fn>. We engineered LSDs to describe important features that
could be leveraged to improve boundary detection. They consist of three
components: size (1-D), offset to center of mass (3-D), and directionality
(6-D).</li>
</ul>
</li>
<li>
<p>Large scale study</p>
<ul>
<li>We conducted a large scale comparative study of competing algorithms on
three large and diverse EM <a href="#datasets">datasets</a>. We demonstate
<a href="#accuracy">competitive results</a> with the current state of the
art while being two orders of magnitude more <a
href="#throughput">efficient</a>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="methods">Methods</h2>
<h3 id="local_shape_descriptors">Local Shape Descriptors</h3>
<p>The intuition behind LSDs is to improve boundary detection by incorporating
statistics describing the local shape of an object close to a boundary. A
similar technique produced superior results over boundary detection alone
<dt-cite key="bai_deep_2017"></dt-cite>. Given a raw EM dataset and unique
neuron labels, we can compute ground truth LSDs in a local window. Specifically,
for each voxel, we grow a gaussian with a fixed radius and intersect it with the
underlying label. We then calculate the center of mass of the intersected region
and compute several statistics between the given voxel and the center of mass.
This is done for all voxels in the window. Perhaps the most important is the
mean offset to center of mass (shown below). This component ensures that a
smooth gradient is maintained within objects while providing sharp contrasts at
object boundaries.</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/lsd_schematic_vertical.jpeg" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<p>Additionally, we calculate two statistics describing the directionality of
neural processes (Covariance and Pearson's correlation coefficient). The former
highlights the orientation of neurons while the latter exposes elongation. The
final component is simply the voxel count inside the intersected region which
translates to the size of the process.</p>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="gt_lsds" />
  <section class="box">
  <label class="box-title" for="gt_lsds">Ground truth LSDs</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%7D%2C%22position%22:%5B98.5%2C98.5%2C98.5%5D%2C%22crossSectionScale%22:1.0456762579315313%2C%22projectionOrientation%22:%5B-0.268303781747818%2C-0.36630383133888245%2C-0.03300556540489197%2C0.8903623819351196%5D%2C%22projectionScale%22:595.714724527316%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/gt_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%280%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%281%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%282%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%5Cn%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B0:3%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/raw%22%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#e100ff%22%2C%22name%22:%22raw%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/gt_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%283%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%284%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%285%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B3:6%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/gt_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%286%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%287%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%288%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B6:9%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/gt_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.8%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20float%20v%20=%20toNormalized%28getDataValue%289%29%29%3B%5Cn%20%20%20%20vec4%20rgba%20=%20vec4%280%2C0%2C0%2C0%29%3B%5Cn%20%20%20%20if%20%28v%20%21=%200.0%29%20%7B%5Cn%20%20%20%20%20%20%20%20rgba%20=%20vec4%28colormapJet%28v%29%2C%201.0%29%3B%5Cn%20%20%20%20%7D%5Cn%20%20%20%20emitRGBA%28rgba%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B9:10%5D%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22LSD%5B9:10%5D%22%2C%22size%22:290%7D%2C%22layout%22:%7B%22type%22:%22row%22%2C%22children%22:%5B%7B%22type%22:%22column%22%2C%22children%22:%5B%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B0:3%5D%22%5D%2C%22layout%22:%22yz%22%7D%2C%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B6:9%5D%22%5D%2C%22layout%22:%22yz%22%7D%5D%7D%2C%7B%22type%22:%22column%22%2C%22children%22:%5B%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B3:6%5D%22%5D%2C%22layout%22:%22yz%22%7D%2C%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B9:10%5D%22%5D%2C%22layout%22:%22yz%22%7D%5D%7D%5D%7D%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<h3 id="network_architectures">Network Architectures</h3>
<div>
  <h3>Popover</h3>
  <p>We implemented the LSDs using three network architectures. All three networks
use a <a data-toggle="popover" data-html="true" data-placement="bottom" title="blah blah blah fooooooooooooooooooooooooooooooooooooooooooo dhhhhhhhhhhhhhhhhhh moveeeeeeeeeeeeeeeeeeeeeeee nnnnnnnnnnnn nnnnnnnnnnnnnnnnnnnn" data-content="<img src='assets/img/unet.jpeg' width='600' />">3D U-Net</a><dt-cite key="funke_large_2019"></dt-cite>. The first is a multitask approach (MtLSD) in which the LSDs are predicted along
with affinities in a single pass, which is a similar approach to the Long Range
affinities: </p></div>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/mtlsd.png" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<p>The second two architectures use an auto-context approach. The first pass predicts LSDs directly from raw data:</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/lsd.png" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<p>The LSD network weights are then used to predict LSDs in a larger region. The
predicted LSDs are fed into a second network (AcLSD) in order to predict
affinities:</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/aclsd.png" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<p>The other auto-context network (AcrLSD) does the same as AcLSD but also includes
a cropped version of the raw data as input to the network, in addition to the
LSDs:</p>
<html>
  <body>
<div style="text-align: center;">
<img class="b-lazy"
src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
data-src="assets/img/acrlsd.png" style="display: block; margin: auto; width: 100%;"/>
<table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
</tr></table>
  </div>
  </body>
</html>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="pred_lsds" />
  <section class="box">
  <label class="box-title" for="pred_lsds">Predicted LSDs</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%7D%2C%22position%22:%5B98.5%2C98.5%2C98.5%5D%2C%22crossSectionScale%22:1.0456762579315313%2C%22projectionOrientation%22:%5B-0.268303781747818%2C-0.36630383133888245%2C-0.03300556540489197%2C0.8903623819351196%5D%2C%22projectionScale%22:595.714724527316%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/pred_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%280%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%281%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%282%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%5Cn%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B0:3%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/raw%22%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#e100ff%22%2C%22name%22:%22raw%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/pred_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%283%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%284%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%285%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B3:6%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/pred_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%286%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%287%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%288%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B6:9%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/pred_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.8%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20float%20v%20=%20toNormalized%28getDataValue%289%29%29%3B%5Cn%20%20%20%20vec4%20rgba%20=%20vec4%280%2C0%2C0%2C0%29%3B%5Cn%20%20%20%20if%20%28v%20%21=%200.0%29%20%7B%5Cn%20%20%20%20%20%20%20%20rgba%20=%20vec4%28colormapJet%28v%29%2C%201.0%29%3B%5Cn%20%20%20%20%7D%5Cn%20%20%20%20emitRGBA%28rgba%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22LSD%5B9:10%5D%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22LSD%5B9:10%5D%22%2C%22size%22:290%7D%2C%22layout%22:%7B%22type%22:%22row%22%2C%22children%22:%5B%7B%22type%22:%22column%22%2C%22children%22:%5B%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B0:3%5D%22%5D%2C%22layout%22:%22yz%22%7D%2C%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B6:9%5D%22%5D%2C%22layout%22:%22yz%22%7D%5D%7D%2C%7B%22type%22:%22column%22%2C%22children%22:%5B%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B3:6%5D%22%5D%2C%22layout%22:%22yz%22%7D%2C%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22LSD%5B9:10%5D%22%5D%2C%22layout%22:%22yz%22%7D%5D%7D%5D%7D%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="gt_vs_pred" />
  <section class="box">
  <label class="box-title" for="gt_vs_pred">Ground truth vs predicted offset
  vectors</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%7D%2C%22position%22:%5B98.5%2C98.5%2C98.5%5D%2C%22crossSectionScale%22:0.6550553083112535%2C%22projectionOrientation%22:%5B-0.268303781747818%2C-0.36630383133888245%2C-0.03300556540489197%2C0.8903623819351196%5D%2C%22projectionScale%22:595.714724527316%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/gt_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%280%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%281%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%282%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%5Cn%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22GT%20LSD%5B0:3%5D%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/raw%22%2C%22tab%22:%22source%22%2C%22annotationColor%22:%22#e100ff%22%2C%22name%22:%22raw%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22zarr://https://raw.githubusercontent.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/draft/assets/zarrs/fib.zarr/pred_lsds_updated%22%2C%22transform%22:%7B%22matrix%22:%5B%5B1%2C0%2C0%2C0%2C52%5D%2C%5B0%2C1%2C0%2C0%2C52%5D%2C%5B0%2C0%2C1%2C0%2C52%5D%2C%5B0%2C0%2C0%2C1%2C0%5D%5D%2C%22outputDimensions%22:%7B%22d0%22:%5B1%2C%22%22%5D%2C%22d1%22:%5B1%2C%22%22%5D%2C%22d2%22:%5B1%2C%22%22%5D%2C%22d3%5E%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22annotationColor%22:%22#0088ff%22%2C%22opacity%22:0.85%2C%22shader%22:%22void%20main%28%29%20%7B%5Cn%20%20%20%20emitRGB%28%5Cn%20%20%20%20%20%20%20%20vec3%28%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%280%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%281%29%29%2C%5Cn%20%20%20%20%20%20%20%20%20%20%20%20toNormalized%28getDataValue%282%29%29%29%5Cn%20%20%20%20%20%20%20%20%29%3B%5Cn%7D%22%2C%22channelDimensions%22:%7B%22d3%5E%22:%5B1%2C%22%22%5D%7D%2C%22name%22:%22Predicted%20LSD%5B0:3%5D%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22GT%20LSD%5B0:3%5D%22%2C%22size%22:290%7D%2C%22layout%22:%7B%22type%22:%22row%22%2C%22children%22:%5B%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22GT%20LSD%5B0:3%5D%22%5D%2C%22layout%22:%22yz%22%7D%2C%7B%22type%22:%22viewer%22%2C%22layers%22:%5B%22raw%22%2C%22Predicted%20LSD%5B0:3%5D%22%5D%2C%22layout%22:%22yz%22%7D%5D%7D%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<hr>
<h2 id="results">Results</h2>
<p>We conducted a large scale study comparing LSD-based methods against three
previous affinity-based methods: (1) direct neighbor and (2) long-range
affinities with mean squared error (MSE) loss, and (3) direct neighbor
affinities with MALIS loss. We also include comparisons against single FFN
segmentations<dt-cite key="januszewski_high-precision_2018"></dt-cite>. The LSD
architectures are described <a href="#network_architectures">here</a>. For a
more detailed overview, see section 3.1 in the paper.</p>
<h3 id="metrics">Metrics</h3>
<p>One challenge of neuron segmentation is finding an evaluation metric which
accurately reflects the quality of a reconstruction. We used two established
metrics: Variation of Information (VoI) and Expected Run-Length (ERL). We also
propose a new metric, which we call the Min-Cut Metric (MCM).</p>
<p><strong>Variation of Information (VoI)<dt-cite key="meila_comparing_2007"></dt-cite></strong> - An established metric which measures the amount of over-segmentation (false
splits) and under-segmentation (false merges) between a proposed segmentation
and ground truth. Taken together, they constitute the VoI Sum. The goal is to
minimize the VoI; a perfect score would be zero, meaning a segmentation
differs as little as possible from the ground truth.</p>
<p><strong>Expected Run Length (ERL)<dt-cite
key="januszewski_high-precision_2018"></dt-cite></strong> - A relatively new metric
which measures the expected length of an error-free path along neurons in a
volume. It is an appealing metric to both engineers and neuroscientists since it
relates segmentation errors to neuron cable length.</p>
<p><strong>Min-Cut Metric (MCM)</strong> - We propose a new metric designed to emulate a human
annotator splitting and merging objects in a segmentation. Specifically, it
gives an approximation of how many clicks are needed to get from a
reconstruction to the correct ground truth (assuming ground truth in
the form of skeletons is available). In a simple case
we have two ground truth skeletons contained inside a falsely merged segment
(1). We perform a min-cut (2), and separate the falsely merged segment into two segments (3).</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/mcm_easy_case.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="100%"><figcaption style="text-align: center;">foo</figcaption></td>
  </tr></table>
  </div>
  </body>
</html>
<p>In a more complex case (1), a min-cut (2) splits the segment but one of the
resulting segments still contains a false merge (3). Another min-cut is
performed (4), resulting in three segments (5). This process is described in
detail in Supplementary section B of the paper.</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/mcm_complex_case.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="100%"><figcaption style="text-align: center;">foo</figcaption></td>
  </tr></table>
  </div>
  </body>
</html>
<h3 id="datasets">Datasets</h3>
<p>We used three large and diverse EM datasets to evaluate all methods and compare
metrics.</p>
<p><strong>Zebrafinch:</strong> The main component of the study was a region taken from songbird
neural tissue <dt-cite key="januszewski_high-precision_2018"></dt-cite>. The
volume comprises ~10<sup>6</sup> cubic microns of raw data. It was imaged using
SBFEM at 9x9x20 (xyx) nanometer resolution. 0.02% of the data was using for
training (33 volumes containing ~200 cubic microns of labeled neurons). 12
manually traced skeletons (13.5 millimeters) were used for network validation
and 50 skeletons (97 millimeters) were used for evaluation. See below for a
visualization:</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/zfinch_dataset.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="100%"><figcaption style="text-align: center;">foo</figcaption></td>
  </tr></table>
  </div>
  </body>
</html>
<p>After training, we predicted in a slightly smaller testing region (~800,000
cubic microns) which we refer to as the Benchmark ROI (region of interest). We created two sets of
supervoxels, one without any masking, and one constrained to a neuropil mask.</p>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="zfinch" />
  <section class="box">
  <label class="box-title" for="zfinch">Zebrafinch</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B2e-8%2C%22m%22%5D%2C%22y%22:%5B2e-8%2C%22m%22%5D%2C%22z%22:%5B2e-8%2C%22m%22%5D%7D%2C%22position%22:%5B2522.154052734375%2C2508.88134765625%2C2990.42919921875%5D%2C%22crossSectionScale%22:16.860758498545437%2C%22projectionOrientation%22:%5B-0.183084174990654%2C-0.3018076419830322%2C0.007873492315411568%2C0.935590922832489%5D%2C%22projectionScale%22:8192%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://j0126-nature-methods-data/GgwKmcKgrcoNxJccKuGIzRnQqfit9hnfK1ctZzNbnuU/rawdata_realigned%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#ff00f7%22%2C%22name%22:%22rawdata_realigned%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://j0126-nature-methods-data/GgwKmcKgrcoNxJccKuGIzRnQqfit9hnfK1ctZzNbnuU/tissue_classification%22%2C%22transform%22:%7B%22outputDimensions%22:%7B%22x%22:%5B2e-8%2C%22m%22%5D%2C%22y%22:%5B2e-8%2C%22m%22%5D%2C%22z%22:%5B2e-8%2C%22m%22%5D%2C%22c%27%22:%5B1%2C%22%22%5D%7D%7D%7D%2C%22tab%22:%22annotations%22%2C%22localDimensions%22:%7B%22c%27%22:%5B1%2C%22%22%5D%7D%2C%22localPosition%22:%5B4%5D%2C%22annotationColor%22:%22#d400ff%22%2C%22shader%22:%22#uicontrol%20vec3%20color%20color%28default=%5C%22magenta%5C%22%29%5Cnvoid%20main%28%29%20%7B%5Cn%20%20emitRGBA%28vec4%28color%2C%20toNormalized%28getDataValue%28%29%29%29%29%3B%5Cn%7D%5Cn%22%2C%22shaderControls%22:%7B%22color%22:%22#0088ff%22%7D%2C%22name%22:%22neuropil%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22neuropil%22%2C%22size%22:649%7D%2C%22layout%22:%224panel%22%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<p>For each affinity-based network, we created segmentations over a range of ROIs in order to assess performance with scale. We then evaluated VoI, ERL, and MCM.</p>
<p><strong>Hemi-Brain:</strong> A volume taken from the <em>Drosophila melanogaster</em> central brain
<dt-cite key="scheffer_connectome_2020"></dt-cite>. It was imaged with FIBSEM at
8 nanometer isotropic resolution and contains a total of 26 teravoxels of image
data. We restricted experiments to the Ellipsoid Body, a neuropil implicated in
spatial navigation <dt-cite key="turner-evans_insect_2016"></dt-cite>. 0.002% of
the data was used for training (~450 cubic microns of labeled neurons). We
restricted segmentations to three ROIs and evaluated against a filtered list of
neurons traced to completion (voxel-based rather than skeletons).</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/hemi_dataset.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="100%"><figcaption style="text-align: center;">foo</figcaption></td>
  </tr></table>
  </div>
  </body>
</html>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="hemi" />
  <section class="box">
  <label class="box-title" for="hemi">Hemi-brain</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B8e-9%2C%22m%22%5D%7D%2C%22position%22:%5B25962.435546875%2C25359.71484375%2C20296.431640625%5D%2C%22crossSectionScale%22:47.72354919422505%2C%22crossSectionDepth%22:-37.62185354999912%2C%22projectionOrientation%22:%5B-0.008687195368111134%2C-0.7010441422462463%2C-0.7130189538002014%2C-0.008097930811345577%5D%2C%22projectionScale%22:21207.72950167948%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/emdata/clahe_yz/jpeg%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#0091ff%22%2C%22name%22:%22emdata%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/rois%22%2C%22subsources%22:%7B%22default%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22pick%22:false%2C%22selectedAlpha%22:0.44%2C%22ignoreNullVisibleSet%22:false%2C%22meshSilhouetteRendering%22:1%2C%22colorSeed%22:2359850678%2C%22segments%22:%5B%2217%22%5D%2C%22segmentQuery%22:%22EB%22%2C%22name%22:%22roi%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22roi%22%2C%22size%22:290%7D%2C%22layout%22:%224panel%22%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<p><strong>FIB-25:</strong> An older dataset taken from the <em>Drosophila</em> visual system <dt-cite
key="takemura_synaptic_2015"></dt-cite>. It contains ~346 gigavoxels of raw EM
data and was imaged with FIBSEM at 8 nanometer resolution. Four volumes
containing ~160 cubic microns of labeled data were used for training. We
predicted inside the entire volume and then restricted supervoxels to an
irregularly shaped neuropil mask. Segmentations were evaluated inside a 13.6
gigavoxel region contained in the bottom half of the full ROI. Similar to the
Hemi-brain, evaluations were done using a filtered list of completely traced
neurons.</p>
<html>
  <body>
<div style="text-align: center;">
  <img class="b-lazy"
    id="neurons"
    src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==
    data-src="assets/img/fib25_dataset.jpeg"
    style="display: block; margin: auto; width: 100%;"/>
  <table style="width: 100%;" cellspacing="0" cellpadding="0"><tr>
  <td width="100%"><figcaption style="text-align: center;">foo</figcaption></td>
  </tr></table>
  </div>
  </body>
</html>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="fib25" />
  <section class="box">
  <label class="box-title" for="fib25">FIB-25</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div class="responsive-container">
    <iframe class="responsive-iframe" src="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B8e-9%2C%22m%22%5D%7D%2C%22position%22:%5B3425.203857421875%2C2993.146240234375%2C3936.575927734375%5D%2C%22crossSectionScale%22:29.90334336415725%2C%22projectionOrientation%22:%5B-0.09087008237838745%2C0.8137544393539429%2C-0.4936932325363159%2C0.292939156293869%5D%2C%22projectionScale%22:11797.675238192334%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-public-data/flyem_fib-25/image%22%2C%22tab%22:%22annotations%22%2C%22annotationColor%22:%22#007bff%22%2C%22name%22:%22image%22%7D%5D%2C%22selectedLayer%22:%7B%22layer%22:%22image%22%7D%2C%22layout%22:%224panel%22%7D" "></iframe>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<h3 id="accuracy">Accuracy</h3>
<h3 id="throughput">Throughput</h3>
<html>
  <body>
  <div class="accordion-container">
  <nav class="accordion arrows">
  <input type="radio" name="accordion" id="block" />
  <section class="box">
  <label class="box-title" for="block">Block-wise processing overview</label>
  <label class="box-close" for="acc-close"></label>
  <div class="box-content"><div>
  <p>Blockwise processing is necessary to process
  such large volumes in a reasonable amount of time. The idea is to chunk up a
  dataset into small blocks which easily fit in memory, and then distribute
  these blocks over many workers (GPUs for prediction, CPUs for
  post-processing). A little bit of context is required to read the input data
  necessary to write the output data. Because of this, only blocks that do not
  touch can be processed simultaneously. Once they finish processing, the next
  set of blocks can begin, and so on until all blocks are complete. Consider
  watershed for a single neuron:
  </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img class="b-lazy"
    src="assets/videos/watershed_separate.gif" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
  <p>The blocks are processed in an alternating fashion ensuring that none touch. Piecing it together gives us the fragments required to generate a neuron:  </p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img class="b-lazy"
    src="assets/videos/watershed_joined.gif" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
  <p>The same logic can be used to stitch the fragments together based on
  the underlying affinities. The result is an agglomerated neuron:</p>
  </div>
  <div class="box-content"><div style="text-align: center;">
    <img class="b-lazy"
    src="assets/videos/agglom_joined.gif" style="display: block; margin: auto; width: 100%;"/>
  </div>
  <div class="box-content"><div>
  <p>This just shows what is happening on an example neuron. In reality every
  object inside every block contained in the full volume is processed in
  parallel. The result is a very efficient processing scheme. For example
  predicting ~115,000 blocks distributed over 100 GPUs took under 2 hours to
  complete (~800,000 total cubic microns). </p>
  </div>
  </div>
  </section>
  <input type="radio" name="accordion" id="acc-close" />
  </nav>
  </div>
  </body>
</html>
<hr>
<h2 id="discussion">Discussion</h2>
<h3 id="metric_eval">Metric Evaluation</h3>
<h3 id="auxiliary_learning">Auxiliary Learning</h3>
<h3 id="auto_context">Auto-Context</h3>
<h3 id="masking">Masking</h3>
<hr>
<h2 id="conclusions">Conclusions</h2>
<p>foo</p>
<hr>
<h2 id="tldr">Tl;dr</h2>
<ul>
<li>
<p>Connectomics is a relatively new field combining neuroscience, microscopy, biology and computer science.</p>
</li>
<li>
<p>The goal is to generate maps of the brain at synaptic resolution. By doing so,
it will hopefully lead to a better understanding of how things work and
subsequently advance medical approaches to various diseases.</p>
</li>
<li>
<p>The datasets required to produce these brain maps are massive since they have
to be imaged at such a high resolution.</p>
</li>
<li>
<p>Manually reconstructing wiring diagrams in the datasets is extremely time consuming and
expensive so there is a great need to automate the process.</p>
</li>
<li>
<p>Reconstructing neurons is challenging because many consecutively correct decisions must be made. Errors can propagate throughout a dataset easily.</p>
</li>
<li>
<p>Methods need to also be computationally efficient and scalable to account
for the size of the data.</p>
</li>
<li>
<p>We present a novel approach to neuron segmentation, Local Shape Descriptors
(LSDs) - a 10-Dimensional embedding used as an auxiliary learning objective
for boundary detection.</p>
</li>
<li>
<p>We find that the LSDs help improve boundaries and subsequent neuron
reconstructions in several large and diverse datasets.</p>
</li>
<li>
<p>They are also two orders of magnitude faster than the current state of the art
approach.</p>
</li>
</ul>
</dt-article>
<dt-appendix>
<h2 id="acknowledgements">Acknowledgements</h2>
<hr>
<h2 id="code">Code</h2>
<hr>
<h2 id="supplementary">Supplementary</h2>
<hr>
<h2 id="references"></h2>
</dt-appendix>
</dt-appendix>
</body>
<script type="text/bibliography">
@article{januszewski_high-precision_2018,
	title = {High-precision automated reconstruction of neurons with flood-filling networks},
	volume = {15},
	issn = {1548-7105},
	doi = {10.1038/s41592-018-0049-4},
	number = {8},
	journal = {Nature Methods},
	author = {Januszewski, Micha and Kornfeld, Jrgen and Li, Peter H. and Pope, Art and Blakely, Tim and Lindsey, Larry and Maitin-Shepard, Jeremy and Tyka, Mike and Denk, Winfried and Jain, Viren},
	year = {2018},
	pages = {605},
        url = {https://www.nature.com/articles/s41592-018-0049-4}
}
@article{zheng_complete_2018,
	title = {A Complete Electron Microscopy Volume of the Brain of Adult Drosophila melanogaster},
	volume = {174},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2018.06.019},
	number = {3},
	journal = {Cell},
	author = {Zheng, Zhihao and Lauritzen, J. Scott and Perlman, Eric and Robinson, Camenzind G. and Nichols, Matthew and Milkie, Daniel and Torrens, Omar and Price, John and Fisher, Corey B. and Sharifi, Nadiya and Calle-Schuler, Steven A. and Kmecova, Lucia and Ali, Iqbal J. and Karsh, Bill and Trautman, Eric T. and Bogovic, John A. and Hanslovsky, Philipp and Jefferis, Gregory S. X. E. and Kazhdan, Michael and Khairy, Khaled and Saalfeld, Stephan and Fetter, Richard D. and Bock, Davi D.},
	year = {2018},
	pages = {730--743},
        url = {https://www.sciencedirect.com/science/article/pii/S0092867418307876}
}
@article{lee_superhuman_2017,
	title = {Superhuman Accuracy on the SNEMI3D Connectomics Challenge},
	url = {http://arxiv.org/abs/1706.00120},
	journal = {arXiv:1706.00120 [cs]},
	author = {Lee, Kisuk and Zung, Jonathan and Li, Peter and Jain, Viren and Seung, H. Sebastian},
	year = {2017},
}
@article{luther_learning_2019,
	title = {Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images},
	journal = {arXiv:1902.00100 [cs]},
	author = {Luther, Kyle and Seung, H. Sebastian},
	year = {2019},
}
@inproceedings{ronneberger_u-net:_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	doi = {10.1007/978-3-319-24574-4_28},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention}  {MICCAI} 2015},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	pages = {234--241}
}
@article{turaga_maximin_2009,
	title = {Maximin affinity learning of image segmentation},
	url = {http://arxiv.org/abs/0911.5372},
	urldate = {2020-10-05},
	journal = {arXiv:0911.5372 [cs]},
	author = {Turaga, Srinivas C. and Briggman, Kevin L. and Helmstaedter, Moritz and Denk, Winfried and Seung, H. Sebastian},
	month = nov,
	year = {2009},
	note = {arXiv: 0911.5372},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}
@article{beier2017multicut,
  title={Multicut brings automated neurite segmentation closer to human performance},
  author={Beier, Thorsten and Pape, Constantin and Rahaman, Nasim and Prange, Timo and Berg, Stuart and Bock, Davi D and Cardona, Albert and Knott, Graham W and Plaza, Stephen M and Scheffer, Louis K and others},
  journal={Nature methods},
  volume={14},
  number={2},
  pages={101--102},
  year={2017},
  publisher={Nature Publishing Group}
}
@inproceedings{cicek_3d_2016,
	title = {3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention}  {MICCAI} 2016},
	author = {\c{C}i\c{c}ek, \"{O}zg\"{u}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	year = {2016},
	pages = {424--432},
}
@article{funke_large_2019,
	title = {Large Scale Image Segmentation with Structured Loss Based Deep Learning for Connectome Reconstruction},
	volume = {41},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Funke, Jan and Tschopp, Fabian and Grisaitis, William and Sheridan, Arlo and Singh, Chandan and Saalfeld, Stephan and Turaga, Srinivas C.},
	year = {2019},
	pages = {1669--1680},
}
@incollection{ciresan_deep_2012,
	title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
	url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
	urldate = {2020-06-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, Jrgen},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2843--2851}
}
@article{turaga_convolutional_2010,
	title = {Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation},
	volume = {22},
	issn = {0899-7667},
	doi = {10.1162/neco.2009.10-08-881},
	number = {2},
	journal = {Neural Computation},
	author = {Turaga, Srinivas C. and Murray, Joseph F. and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H. Sebastian},
	month = feb,
	year = {2010},
	note = {Conference Name: Neural Computation},
	pages = {511--538}
}
@inproceedings{bai_deep_2017,
	title = {Deep Watershed Transform for Instance Segmentation},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Bai_Deep_Watershed_Transform_CVPR_2017_paper.html},
	urldate = {2020-06-30},
	author = {Bai, Min and Urtasun, Raquel},
	year = {2017},
	pages = {5221--5229}
}
@article{takemura_synaptic_2015,
	title = {Synaptic circuits and their variations within different columns in the visual system of Drosophila},
	volume = {112},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4640747/},
	doi = {10.1073/pnas.1509820112},
	number = {44},
	urldate = {2020-07-22},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Takemura, Shin-ya and Xu, C. Shan and Lu, Zhiyuan and Rivlin, Patricia K. and Parag, Toufiq and Olbris, Donald J. and Plaza, Stephen and Zhao, Ting and Katz, William T. and Umayam, Lowell and Weaver, Charlotte and Hess, Harald F. and Horne, Jane Anne and Nunez-Iglesias, Juan and Aniceto, Roxanne and Chang, Lei-Ann and Lauchie, Shirley and Nasca, Ashley and Ogundeyi, Omotara and Sigmund, Christopher and Takemura, Satoko and Tran, Julie and Langille, Carlie and Le Lacheur, Kelsey and McLin, Sari and Shinomiya, Aya and Chklovskii, Dmitri B. and Meinertzhagen, Ian A. and Scheffer, Louis K.},
	month = nov,
	year = {2015},
	pmid = {26483464},
	pmcid = {PMC4640747},
	pages = {13711--13716},
}
@article{lee_learning_2019,
	title = {Learning Dense Voxel Embeddings for 3D Neuron Reconstruction},
	url = {http://arxiv.org/abs/1909.09872},
	urldate = {2020-09-02},
	journal = {arXiv:1909.09872 [cs]},
	author = {Lee, Kisuk and Lu, Ran and Luther, Kyle and Seung, H. Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.09872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{tu_auto-context_2010,
	title = {Auto-{Context} and {Its} {Application} to {High}-{Level} {Vision} {Tasks} and {3D} {Brain} {Image} {Segmentation}},
	volume = {32},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2009.186},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Tu, Zhuowen and Bai, Xiang},
	month = oct,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1744--1757}
}
@article{meila_comparing_2007,
	title = {Comparing clusteringsan information based distance},
	volume = {98},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X06002016},
	doi = {10.1016/j.jmva.2006.11.013},
	language = {en},
	number = {5},
	urldate = {2020-09-10},
	journal = {Journal of Multivariate Analysis},
	author = {Meil, Marina},
	month = may,
	year = {2007},
	keywords = {Agreement measures, Clustering, Comparing partitions, Information theory, Mutual information, Similarity measures},
	pages = {873--895}
}
@incollection{ferrari_mutex_2018,
	address = {Cham},
	title = {The Mutex Watershed: Efficient, Parameter-Free Image Partitioning},
	volume = {11208},
	isbn = {978-3-030-01224-3 978-3-030-01225-0},
	shorttitle = {The {Mutex} {Watershed}},
	url = {http://link.springer.com/10.1007/978-3-030-01225-0_34},
	language = {en},
	urldate = {2020-09-15},
	booktitle = {Computer {Vision}  {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Wolf, Steffen and Pape, Constantin and Bailoni, Alberto and Rahaman, Nasim and Kreshuk, Anna and Kthe, Ullrich and Hamprecht, Fred A.},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01225-0_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {571--587}
}
@article{li_automated_2019,
	title = {Automated Reconstruction of a Serial-Section EM Drosophila Brain with Flood-Filling Networks and Local Realignment},
	copyright = { 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/605634v1},
	doi = {10.1101/605634},
	language = {en},
	urldate = {2020-09-16},
	journal = {bioRxiv},
	author = {Li, Peter H. and Lindsey, Larry F. and Januszewski, Micha and Zheng, Zhihao and Bates, Alexander Shakeel and Taisz, Istvn and Tyka, Mike and Nichols, Matthew and Li, Feng and Perlman, Eric and Maitin-Shepard, Jeremy and Blakely, Tim and Leavitt, Laramie and Jefferis, Gregory S. X. E. and Bock, Davi and Jain, Viren},
	month = apr,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {605634}
}
@article{abbott_mind_2020,
	title = {The Mind of a Mouse},
	volume = {182},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867420310011},
	doi = {10.1016/j.cell.2020.08.010},
	language = {en},
	number = {6},
	urldate = {2020-09-17},
	journal = {Cell},
	author = {Abbott, Larry F. and Bock, Davi D. and Callaway, Edward M. and Denk, Winfried and Dulac, Catherine and Fairhall, Adrienne L. and Fiete, Ila and Harris, Kristen M. and Helmstaedter, Moritz and Jain, Viren and Kasthuri, Narayanan and LeCun, Yann and Lichtman, Jeff W. and Littlewood, Peter B. and Luo, Liqun and Maunsell, John H. R. and Reid, R. Clay and Rosen, Bruce R. and Rubin, Gerald M. and Sejnowski, Terrence J. and Seung, H. Sebastian and Svoboda, Karel and Tank, David W. and Tsao, Doris and Van Essen, David C.},
	month = sep,
	year = {2020},
	pages = {1372--1376}
}
@article{turner-evans_neuroanatomical_2020,
	title = {The Neuroanatomical Ultrastructure and Function of a Biological Ring Attractor},
	volume = {0},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(20)30613-9},
	doi = {10.1016/j.neuron.2020.08.006},
	language = {English},
	number = {0},
	urldate = {2020-09-17},
	journal = {Neuron},
	author = {Turner-Evans, Daniel B. and Jensen, Kristopher T. and Ali, Saba and Paterson, Tyler and Sheridan, Arlo and Ray, Robert P. and Wolff, Tanya and Lauritzen, J. Scott and Rubin, Gerald M. and Bock, Davi D. and Jayaraman, Vivek},
	month = sep,
	year = {2020},
	pmid = {32916090},
	note = {Publisher: Elsevier},
	keywords = {behavior, central complex, Drosophila, electron microscopy, head direction, navigation, network dynamics, neural circuit, RNA-seq, two-photon calcium imaging}
}
@article{bates_complete_2020,
	title = {Complete Connectomic Reconstruction of Olfactory Projection Neurons in the Fly Brain},
	volume = {30},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S0960982220308587},
	doi = {10.1016/j.cub.2020.06.042},
	language = {en},
	number = {16},
	urldate = {2020-09-17},
	journal = {Current Biology},
	author = {Bates, Alexander S. and Schlegel, Philipp and Roberts, Ruairi J. V. and Drummond, Nikolas and Tamimi, Imaan F. M. and Turnbull, Robert and Zhao, Xincheng and Marin, Elizabeth C. and Popovici, Patricia D. and Dhawan, Serene and Jamasb, Arian and Javier, Alexandre and Serratosa Capdevila, Laia and Li, Feng and Rubin, Gerald M. and Waddell, Scott and Bock, Davi D. and Costa, Marta and Jefferis, Gregory S. X. E.},
	month = aug,
        year = {2020},
	keywords = {connectomics, Drosophila, EM, memory, neuroanatomy, olfaction, synapses},
	pages = {3183--3199.e6}
}
@article{motta_dense_2019,
	title = {Dense connectomic reconstruction in layer 4 of the somatosensory cortex},
	volume = {366},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/366/6469/eaay3134},
	doi = {10.1126/science.aay3134},
	number = {6469},
	journal = {Science},
	author = {Motta, Alessandro and Berning, Manuel and Boergens, Kevin M. and Staffler, Benedikt and Beining, Marcel and Loomba, Sahil and Hennig, Philipp and Wissler, Heiko and Helmstaedter, Moritz},
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/366/6469/eaay3134.full.pdf}
}
@article{schlegel_synaptic_2016,
	title = {Synaptic transmission parallels neuromodulation in a central food-intake circuit},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.16799},
	doi = {10.7554/eLife.16799},
	urldate = {2020-09-17},
	journal = {eLife},
	author = {Schlegel, Philipp and Texada, Michael J and Miroschnikow, Anton and Schoofs, Andreas and Hckesfeld, Sebastian and Peters, Marc and Schneider-Mizell, Casey M and Lacin, Haluk and Li, Feng and Fetter, Richard D and Truman, James W and Cardona, Albert and Pankratz, Michael J},
	editor = {Calabrese, Ronald L},
	month = nov,
	year = {2016},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {acetylcholine, co-transmission, endocrine, hugin, neuromedin, neuropeptides},
	pages = {e16799}
}
@article{schneider-mizell_quantitative_2016,
	title = {Quantitative neuroanatomy for connectomics in Drosophila},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.12059},
	doi = {10.7554/eLife.12059},
	urldate = {2020-09-17},
	journal = {eLife},
	author = {Schneider-Mizell, Casey M and Gerhard, Stephan and Longair, Mark and Kazimiers, Tom and Li, Feng and Zwart, Maarten F and Champion, Andrew and Midgley, Frank M and Fetter, Richard D and Saalfeld, Stephan and Cardona, Albert},
	editor = {Calabrese, Ronald L},
	month = mar,
	year = {2016},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {connectomics, neuroanatomy, proprioception},
	pages = {e12059}
}
@article{briggman_volume_2012,
	series = {Neurotechnology},
	title = {Volume electron microscopy for neuronal circuit reconstruction},
	volume = {22},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438811001887},
	doi = {10.1016/j.conb.2011.10.022},
	language = {en},
	number = {1},
	urldate = {2020-09-17},
	journal = {Current Opinion in Neurobiology},
	author = {Briggman, Kevin L and Bock, Davi D},
	month = feb,
	year = {2012},
	pages = {154--161}
}
@article{dorkenwald_binary_2019,
	title = {Binary and analog variation of synapses between cortical pyramidal neurons},
	copyright = { 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2019.12.29.890319v1},
	doi = {10.1101/2019.12.29.890319},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Dorkenwald, Sven and Turner, Nicholas L. and Macrina, Thomas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Bodor, Agnes L. and Bleckert, Adam A. and Brittain, Derrick and Kemnitz, Nico and Silversmith, William M. and Ih, Dodam and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Yu, Szi-Chieh and Popovych, Sergiy and Wong, William and Castro, Manuel and Jordan, Chris S. and Wilson, Alyssa M. and Froudarakis, Emmanouil and Buchanan, JoAnn and Takeno, Marc and Torres, Russel and Mahalingam, Gayathri and Collman, Forrest and Schneider-Mizell, Casey and Bumbarger, Daniel J. and Li, Yang and Becker, Lynne and Suckow, Shelby and Reimer, Jacob and Tolias, Andreas S. and Costa, Nuno Maarico da and Reid, R. Clay and Seung, H. Sebastian},
	month = dec,
	year = {2019},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {2019.12.29.890319}
}
@article{maniates-selvin_reconstruction_2020,
	title = {Reconstruction of motor control circuits in adult Drosophila using automated transmission electron microscopy},
	copyright = { 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.01.10.902478v1},
	doi = {10.1101/2020.01.10.902478},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Maniates-Selvin, Jasper T. and Hildebrand, David Grant Colburn and Graham, Brett J. and Kuan, Aaron T. and Thomas, Logan A. and Nguyen, Tri and Buhmann, Julia and Azevedo, Anthony W. and Shanny, Brendan L. and Funke, Jan and Tuthill, John C. and Lee, Wei-Chung Allen},
	month = jan,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory Section: New Results},
	pages = {2020.01.10.902478}
}
@article{heinrich_synaptic_2018,
	title = {Synaptic Cleft Segmentation in Non-Isotropic Volume Electron Microscopy of the Complete Drosophila Brain},
	url = {http://arxiv.org/abs/1805.02718},
	urldate = {2020-09-17},
	journal = {arXiv:1805.02718 [cs]},
	author = {Heinrich, Larissa and Funke, Jan and Pape, Constantin and Nunez-Iglesias, Juan and Saalfeld, Stephan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.02718},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{scheffer_connectome_2020,
	title = {A Connectome and Analysis of the Adult Drosophila Central Brain},
	copyright = { 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.07.030213v1},
	doi = {10.1101/2020.04.07.030213},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Scheffer, Louis K. and Xu, C. Shan and Januszewski, Michal and Lu, Zhiyuan and Takemura, Shin-ya and Hayworth, Kenneth J. and Huang, Gary B. and Shinomiya, Kazunori and Maitin-Shepard, Jeremy and Berg, Stuart and Clements, Jody and Hubbard, Philip and Katz, William and Umayam, Lowell and Zhao, Ting and Ackerman, David and Blakely, Tim and Bogovic, John and Dolafi, Tom and Kainmueller, Dagmar and Kawase, Takashi and Khairy, Khaled A. and Leavitt, Laramie and Li, Peter H. and Lindsey, Larry and Neubarth, Nicole and Olbris, Donald J. and Otsuna, Hideo and Trautman, Eric T. and Ito, Masayoshi and Goldammer, Jens and Wolff, Tanya and Svirskas, Robert and Schlegel, Philipp and Neace, Erika R. and Knecht, Christopher J. and Alvarado, Chelsea X. and Bailey, Dennis A. and Ballinger, Samantha and Borycz, Jolanta A. and Canino, Brandon S. and Cheatham, Natasha and Cook, Michael and Dreher, Marisa and Duclos, Octave and Eubanks, Bryon and Fairbanks, Kelli and Finley, Samantha and Forknall, Nora and Francis, Audrey and Hopkins, Gary Patrick and Joyce, Emily M. and Kim, SungJin and Kirk, Nicole A. and Kovalyak, Julie and Lauchie, Shirley A. and Lohff, Alanna and Maldonado, Charli and Manley, Emily A. and McLin, Sari and Mooney, Caroline and Ndama, Miatta and Ogundeyi, Omotara and Okeoma, Nneoma and Ordish, Christopher and Padilla, Nicholas and Patrick, Christopher and Paterson, Tyler and Phillips, Elliott E. and Phillips, Emily M. and Rampally, Neha and Ribeiro, Caitlin and Robertson, Madelaine K. and Rymer, Jon Thomson and Ryan, Sean M. and Sammons, Megan and Scott, Anne K. and Scott, Ashley L. and Shinomiya, Aya and Smith, Claire and Smith, Kelsey and Smith, Natalie L. and Sobeski, Margaret A. and Suleiman, Alia and Swift, Jackie and Takemura, Satoko and Talebi, Iris and Tarnogorska, Dorota and Tenshaw, Emily and Tokhi, Temour and Walsh, John J. and Yang, Tansy and Horne, Jane Anne and Li, Feng and Parekh, Ruchi and Rivlin, Patricia K. and Jayaraman, Vivek and Ito, Kei and Saalfeld, Stephan and George, Reed and Meinertzhagen, Ian A. and Rubin, Gerald M. and Hess, Harald F. and Jain, Viren and Plaza, Stephen M.},
	month = apr,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.04.07.030213}
}
@inproceedings{kreshuk_who_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Who Is Talking to Whom: Synaptic Partner Detection in Anisotropic Volumes of Insect Brain},
	isbn = {978-3-319-24553-9},
	shorttitle = {Who {Is} {Talking} to {Whom}},
	doi = {10.1007/978-3-319-24553-9_81},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} -- {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Kreshuk, Anna and Funke, Jan and Cardona, Albert and Hamprecht, Fred A.},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro},
	year = {2015},
	keywords = {Circuit reconstruction, electron microscopy, graphical model},
	pages = {661--668}
}
@article{buhmann_synaptic_2018,
	title = {Synaptic partner prediction from point annotations in insect brains},
	url = {http://arxiv.org/abs/1806.08205},
	urldate = {2020-09-17},
	journal = {arXiv:1806.08205 [cs]},
	author = {Buhmann, Julia and Krause, Renate and Lentini, Rodrigo Ceballos and Eckstein, Nils and Cook, Matthew and Turaga, Srinivas and Funke, Jan},
	month = jul,
	year = {2018},
	note = {arXiv: 1806.08205},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{buhmann_automatic_2020,
	title = {Automatic Detection of Synaptic Partners in a Whole-Brain Drosophila EM Dataset},
	copyright = { 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2019.12.12.874172v2},
	doi = {10.1101/2019.12.12.874172},
	language = {en},
	urldate = {2020-09-17},
	journal = {bioRxiv},
	author = {Buhmann, Julia and Sheridan, Arlo and Gerhard, Stephan and Krause, Renate and Nguyen, Tri and Heinrich, Larissa and Schlegel, Philipp and Lee, Wei-Chung Allen and Wilson, Rachel and Saalfeld, Stephan and Jefferis, Gregory and Bock, Davi and Turaga, Srinivas and Cook, Matthew and Funke, Jan},
	month = mar,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2019.12.12.874172}
}
@article{huang_fully-automatic_2018,
	title = {Fully-Automatic Synapse Prediction and Validation on a Large Data Set},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00087/full},
	doi = {10.3389/fncir.2018.00087},
	language = {English},
	urldate = {2020-09-17},
	journal = {Frontiers in Neural Circuits},
	author = {Huang, Gary B. and Scheffer, Louis K. and Plaza, Stephen M.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {connectomics, deep learning, Drosophila, Evaluation, synapse prediction}
}
@article{plaza_analyzing_2018,
	title = {Analyzing Image Segmentation for Connectomics},
	volume = {12},
	issn = {1662-5110},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243088/},
	doi = {10.3389/fncir.2018.00102},
	urldate = {2020-09-23},
	journal = {Frontiers in Neural Circuits},
	author = {Plaza, Stephen M. and Funke, Jan},
	month = nov,
	year = {2018},
	pmid = {30483069},
	pmcid = {PMC6243088}
}
@article{kornfeld_em_2017,
	title = {EM connectomics reveals axonal target variation in a sequence-generating network},
	volume = {6},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.24364},
	doi = {10.7554/eLife.24364},
	urldate = {2020-10-05},
	journal = {eLife},
	author = {Kornfeld, Jrgen and Benezra, Sam E and Narayanan, Rajeevan T and Svara, Fabian and Egger, Robert and Oberlaender, Marcel and Denk, Winfried and Long, Michael A},
	editor = {Svoboda, Karel},
	month = mar,
	year = {2017},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {birdsong, connectomics, neural sequences, synfire chains, zebra finch},
	pages = {e24364}
}
@article{schneider-mizell_chandelier_2020,
	title = {Chandelier cell anatomy and function reveal a variably distributed but common signal},
	copyright = { 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.31.018952v1},
	doi = {10.1101/2020.03.31.018952},
	language = {en},
	urldate = {2020-10-05},
	journal = {bioRxiv},
	author = {Schneider-Mizell, Casey M. and Bodor, Agnes L. and Collman, Forrest and Brittain, Derrick and Bleckert, Adam A. and Dorkenwald, Sven and Turner, Nicholas L. and Macrina, Thomas and Lee, Kisuk and Lu, Ran and Wu, Jingpeng and Zhuang, Jun and Nandi, Anirban and Hu, Brian and Buchanan, JoAnn and Takeno, Marc M. and Torres, Russel and Mahalingam, Gayathri and Bumbarger, Daniel J. and Li, Yang and Chartrand, Tom and Kemnitz, Nico and Silversmith, William M. and Ih, Dodam and Zung, Jonathan and Zlateski, Aleksandar and Tartavull, Ignacio and Popovych, Sergiy and Wong, William and Castro, Manuel and Jordan, Chris S. and Froudarakis, Emmanouil and Becker, Lynne and Suckow, Shelby and Reimer, Jacob and Tolias, Andreas S. and Anastassiou, Costas and Seung, H. Sebastian and Reid, R. Clay and Costa, Nuno Maarico da},
	month = apr,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.03.31.018952}
}
@article{yin_petascale_2020,
	title = {A petascale automated imaging pipeline for mapping neuronal circuits with high-throughput transmission electron microscopy},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-18659-3},
	doi = {10.1038/s41467-020-18659-3},
	language = {en},
	number = {1},
	urldate = {2020-10-05},
	journal = {Nature Communications},
	author = {Yin, Wenjing and Brittain, Derrick and Borseth, Jay and Scott, Marie E. and Williams, Derric and Perkins, Jedediah and Own, Christopher S. and Murfitt, Matthew and Torres, Russel M. and Kapner, Daniel and Mahalingam, Gayathri and Bleckert, Adam and Castelli, Daniel and Reid, David and Lee, Wei-Chung Allen and Graham, Brett J. and Takeno, Marc and Bumbarger, Daniel J. and Farrell, Colin and Reid, R. Clay and da Costa, Nuno Macarico},
	month = oct,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {4949}
}
@article{rand_objective_1971,
	title = {Objective Criteria for the Evaluation of Clustering Methods},
	volume = {66},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2284239},
	doi = {10.2307/2284239},
	abstract = {Many intuitively appealing methods have been suggested for clustering data, however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
	number = {336},
	urldate = {2020-10-05},
	journal = {Journal of the American Statistical Association},
	author = {Rand, William M.},
	year = {1971},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {846--850}
}
@article{turner-evans_insect_2016,
	title = {The insect central complex},
	volume = {26},
	issn = {09609822},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0960982216303232},
	doi = {10.1016/j.cub.2016.04.006},
	language = {en},
	number = {11},
	urldate = {2020-10-08},
	journal = {Current Biology},
	author = {Turner-Evans, Daniel B. and Jayaraman, Vivek},
	month = jun,
	year = {2016},
	pages = {R453--R457},
	file = {Turner-Evans and Jayaraman - 2016 - The insect central complex.pdf:/Users/ArloS/Zotero/storage/LR7BU6C3/Turner-Evans and Jayaraman - 2016 - The insect central complex.pdf:application/pdf}
}
@incollection{maitin-shepard_combinatorial_2016,
	title = {Combinatorial Energy Learning for Image Segmentation},
	url = {http://papers.nips.cc/paper/6595-combinatorial-energy-learning-for-image-segmentation.pdf},
	urldate = {2020-10-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Maitin-Shepard, Jeremy B and Jain, Viren and Januszewski, Michal and Li, Peter and Abbeel, Pieter},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1966--1974},
	file = {NIPS Full Text PDF:/Users/ArloS/Zotero/storage/VUAW9P2C/Maitin-Shepard et al. - 2016 - Combinatorial Energy Learning for Image Segmentati.pdf:application/pdf;NIPS Snapshot:/Users/ArloS/Zotero/storage/A5AP8YLR/6595-combinatorial-energy-learning-for-image-segmentation.html:text/html}
}
@article{nguyen2020daisy,
	title={Daisy: A Library for Block-Wise Task Scheduling for Large nD Volumes},
	author={Nguyen, Tri and Malin-Mayor, Caroline and Patton, William and Funke, Jan},
	journal={in preparation},
	year={2020}
}
@article{funke_ted_2017,
	series = {Image {Processing} for {Biologists}},
	title = {TED: A Tolerant Edit Distance for segmentation evaluation},
	volume = {115},
	issn = {1046-2023},
	shorttitle = {{TED}},
	url = {http://www.sciencedirect.com/science/article/pii/S1046202316305072},
	doi = {10.1016/j.ymeth.2016.12.013},
	language = {en},
	urldate = {2020-10-28},
	journal = {Methods},
	author = {Funke, Jan and Klein, Jonas and Moreno-Noguer, Francesc and Cardona, Albert and Cook, Matthew},
	month = feb,
	year = {2017},
	keywords = {Computer vision, Electron microscopy, Evaluation, Learning, Neuron segmentation, Segmentation},
	pages = {119--127},
	file = {ScienceDirect Full Text PDF:/Users/ArloS/Zotero/storage/P4UBBSN6/Funke et al. - 2017 - TED A Tolerant Edit Distance for segmentation eva.pdf:application/pdf;ScienceDirect Snapshot:/Users/ArloS/Zotero/storage/YSBFR2GJ/S1046202316305072.html:text/html}
}
@inproceedings{plaza_focused_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Focused Proofreading to Reconstruct Neural Connectomes from EM Images at Scale},
	isbn = {978-3-319-46976-8},
	doi = {10.1007/978-3-319-46976-8_26},
	language = {en},
	booktitle = {Deep {Learning} and {Data} {Labeling} for {Medical} {Applications}},
	publisher = {Springer International Publishing},
	author = {Plaza, Stephen M.},
	editor = {Carneiro, Gustavo and Mateus, Diana and Peter, Loc and Bradley, Andrew and Tavares, Joo Manuel R. S. and Belagiannis, Vasileios and Papa, Joo Paulo and Nascimento, Jacinto C. and Loog, Marco and Lu, Zhi and Cardoso, Jaime S. and Cornebise, Julien},
	year = {2016},
	keywords = {Automatic Segmentation, Edge Probability, Initial Segmentation, Optic Lobe, Synaptic Connection},
	pages = {249--258},
	file = {Springer Full Text PDF:/Users/ArloS/Zotero/storage/JNG5MEDQ/Plaza - 2016 - Focused Proofreading to Reconstruct Neural Connect.pdf:application/pdf}
}
@article{gallusser_2020,
  title={Deep-Learning-Based Automatic Organelle Segmentation in 3D Electron Microscopy Datasets},
  author={Gallusser, Benjamin and Vadakkan, Tegy John and Sahasrabudhe, Mihir and Di Caprio, Giuseppe and Kirchhausen, Tom},
  journal={in preparation},
  year={2020}
}
@article{hildebrand_whole-brain_2017,
	title = {Whole-brain serial-section electron microscopy in larval zebrafish},
	volume = {545},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature22356},
	doi = {10.1038/nature22356},
	abstract = {A complete larval zebrafish brain is examined and its myelinated axons reconstructed using serial-section electron microscopy, revealing remarkable symmetry and providing a valuable resource.},
	language = {en},
	number = {7654},
	urldate = {2020-11-11},
	journal = {Nature},
	author = {Hildebrand, David Grant Colburn and Cicconet, Marcelo and Torres, Russel Miguel and Choi, Woohyuk and Quan, Tran Minh and Moon, Jungmin and Wetzel, Arthur Willis and Scott Champion, Andrew and Graham, Brett Jesse and Randlett, Owen and Plummer, George Scott and Portugues, Ruben and Bianco, Isaac Henry and Saalfeld, Stephan and Baden, Alexander David and Lillaney, Kunal and Burns, Randal and Vogelstein, Joshua Tzvi and Schier, Alexander Franz and Lee, Wei-Chung Allen and Jeong, Won-Ki and Lichtman, Jeff William and Engert, Florian},
	month = may,
	year = {2017},
	note = {Number: 7654
Publisher: Nature Publishing Group},
	pages = {345--349},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/MXUPZ29N/Hildebrand et al. - 2017 - Whole-brain serial-section electron microscopy in .pdf:application/pdf;Snapshot:/Users/ArloS/Zotero/storage/NYCZKHWR/nature22356.html:text/html}
}
@article{saalfeld2009catmaid,
	title={CATMAID: collaborative annotation toolkit for massive amounts of image data},
	author={Saalfeld, Stephan and Cardona, Albert and Hartenstein, Volker and Toman{\v{c}}{\'a}k, Pavel},
	journal={Bioinformatics},
	volume={25},
	number={15},
	pages={1984--1986},
	year={2009},
	publisher={Oxford University Press}
}
@article{boergens_webknossos_2017,
	title = {webKnossos: efficient online {3D} data annotation for connectomics},
	volume = {14},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	shorttitle = {{webKnossos}},
	url = {https://www.nature.com/articles/nmeth.4331},
	doi = {10.1038/nmeth.4331},
	language = {en},
	number = {7},
	urldate = {2020-11-16},
	journal = {Nature Methods},
	author = {Boergens, Kevin M. and Berning, Manuel and Bocklisch, Tom and Brunlein, Dominic and Drawitsch, Florian and Frohnhofen, Johannes and Herold, Tom and Otto, Philipp and Rzepka, Norman and Werkmeister, Thomas and Werner, Daniel and Wiese, Georg and Wissler, Heiko and Helmstaedter, Moritz},
	month = jul,
	year = {2017},
	note = {Number: 7
Publisher: Nature Publishing Group},
	pages = {691--694},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/VDHQQKL2/Boergens et al. - 2017 - webKnossos efficient online 3D data annotation fo.pdf:application/pdf;Snapshot:/Users/ArloS/Zotero/storage/CQJI3QWJ/nmeth.html:text/html}
}
@article{berger_vast_2018,
	title = {VAST (Volume Annotation and Segmentation Tool): Efficient
        Manual and Semi-Automatic Labeling of Large 3D Image Stacks},
	volume = {12},
	issn = {1662-5110},
	shorttitle = {{VAST} ({Volume} {Annotation} and {Segmentation} {Tool})},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00088/full},
	doi = {10.3389/fncir.2018.00088},
	language = {English},
	urldate = {2020-11-16},
	journal = {Frontiers in Neural Circuits},
	author = {Berger, Daniel R. and Seung, H. Sebastian and Lichtman, Jeff W.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {CLEM, connectomics, proofreading, segmentation, serial section electron microscopy, tracing, TrakEM2, visualization, Voxel},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/DBQWS7DN/Berger et al. - 2018 - VAST (Volume Annotation and Segmentation Tool) Ef.pdf:application/pdf}
}
@article{zhao_neutu_2018,
	title = {NeuTu: Software for Collaborative, Large-Scale, Segmentation-Based Connectome Reconstruction},
	volume = {12},
	issn = {1662-5110},
	shorttitle = {{NeuTu}},
	url = {https://www.frontiersin.org/articles/10.3389/fncir.2018.00101/full},
	doi = {10.3389/fncir.2018.00101},
	language = {English},
	urldate = {2020-11-16},
	journal = {Frontiers in Neural Circuits},
	author = {Zhao, Ting and Olbris, Donald J. and Yu, Yang and Plaza, Stephen M.},
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {connectome, Electron microscopy, NeuTu, proofreading, Segmentation (Image processing)},
	file = {Full Text PDF:/Users/ArloS/Zotero/storage/4WPFZEFA/Zhao et al. - 2018 - NeuTu Software for Collaborative, Large-Scale, Se.pdf:application/pdf}
}


</script>
<!--


-->
<script language="javascript" type="text/javascript" src="lib/p5.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/p5.dom.js"></script>
<script language="javascript" type="text/javascript" src="lib/numjs.js"></script>
<script src="lib/blazy.js"></script>
<script language="javascript" type="text/javascript" src="lib/jquery-1.12.4.min.js"></script>
<script language="javascript" type="text/javascript" src="lib/utils.js"></script>
